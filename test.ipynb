{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from anthropic import AsyncAnthropic\n",
    "import torch\n",
    "from retriever import ChainOfThoughtRetriever\n",
    "from preprocessing import AsyncDocumentProcessor\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_search_system(processed_documents, api_key):\n",
    "    # Set up the embedding model\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    # Determine the best available device\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        embedding_model.to('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        embedding_model.to('mps')\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        embedding_model.to('cpu')\n",
    "    \n",
    "    # Initialize the Anthropic client\n",
    "    anthropic_client = AsyncAnthropic(api_key=api_key)\n",
    "    \n",
    "    # Create the retriever\n",
    "    retriever = ChainOfThoughtRetriever(\n",
    "        documents=processed_documents,\n",
    "        embedding_model=embedding_model,\n",
    "        anthropic_client=anthropic_client,\n",
    "        device=device,  # Pass the device explicitly\n",
    "        max_iterations=1,\n",
    "        results_per_step=5\n",
    "    )\n",
    "        # In your main code, after initializing the retriever\n",
    "    # print(f\"FAISS index dimension: {retriever.combined_faiss_index.d}\")\n",
    "    print(f\"Embedding model dimension: {retriever.embedding_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:Using Apple Silicon with Metal Performance Shaders\n",
      "INFO:preprocessing:Selected device for computation: mps\n",
      "INFO:preprocessing:Using device: mps\n",
      "INFO:preprocessing:Initializing with 7 processes\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing model name: sentence-transformers/all-mpnet-base-v2\n",
      "Document: /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt\n",
      "Chunk embedding shape: ()\n",
      "Embedding model dimension: 768\n",
      "Getting results for query: what are the use cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for query: \"what are some potential use cases for a local file search engine that uses natural language processing and semantic embeddings?\"\n",
      "\n",
      "CONFIDENCE: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for query: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach?\"\n",
      "\n",
      "CONFIDENCE: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for query: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach? How would the system handle different file types and formats, and what specific NLP techniques or models would be used?\"\n",
      "\n",
      "CONFIDENCE: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for query: \"What are the key use cases for a local file search engine leveraging natural language processing and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach? How would it handle different file formats? What specific NLP models or techniques could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "processor = AsyncDocumentProcessor()\n",
    "print(f\"Preprocessing model name: {processor.embedding_model_name}\")\n",
    "output_dir = Path(\"processed_documents\")\n",
    "\n",
    "# Load indices from disk\n",
    "await processor.load_indices(str(output_dir))\n",
    "\n",
    "processed_documents = processor.documents # Your preprocessed documents\n",
    "# After loading your documents\n",
    "for doc_path, doc_data in processed_documents.items():\n",
    "    for chunk in doc_data['chunks']:\n",
    "        embedding = chunk['embedding']\n",
    "        print(f\"Document: {doc_path}\")\n",
    "        print(f\"Chunk embedding shape: {np.array(embedding).shape}\")\n",
    "        break  # Just check the first chunk\n",
    "    break  # Just check the first document\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = await initialize_search_system(\n",
    "    processed_documents=processed_documents,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Perform a search\n",
    "results, reasoning_steps = await retriever.search(\n",
    "    \"what are the use cases\",\n",
    "    return_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [\n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G..., \n",
      "Search Result #12\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio..., \n",
      "Search Result #4\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa..., \n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc..., \n",
      "Search Result #3\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha...]\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "query: what are the use cases\n",
      "results\n",
      "\n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G...\n",
      "\n",
      "Search Result #12\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio...\n",
      "\n",
      "Search Result #4\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa...\n",
      "\n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc...\n",
      "\n",
      "Search Result #3\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Only one specific use case is provided (retrieving class notes). Additional use cases in different domains or scenarios would be helpful.\n",
      "  • - Details on how the search engine would handle different file types or formats are missing.\n",
      "  • - Information on the user interface and how users would interact with the search engine is lacking.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"what are some potential use cases for a local file search engine that uses natural language processing and semantic embeddings?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results only provide a single use case for the proposed search engine, which is insuffic...\n",
      "combined_socre: {'2': 0.0, '3': 0.19389090909090906, '4': 0.006691616535523905, '11': -6.855537799072265, '12': 0.49767500000000003, '20': 0.48078688186459034, '22': 0.5914301043219077}\n",
      "\n",
      "Step 1\n",
      "query: \"what are some potential use cases for a local file search engine that uses natural language processing and semantic embeddings?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #30\n",
      "Score: 0.032\n",
      "Source: 2.pdf\n",
      "Text: TCP/IP configurations. Building upon this backgrou...\n",
      "\n",
      "Search Result #3\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive examples of use cases across different domains (e.g., personal, enterprise, research)\n",
      "  • - Details on how the search engine would handle different file types or formats\n",
      "  • - Potential challenges or limitations of using language models for local file search\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide a good overview of the problem and objectives but lack comprehensive cov...\n",
      "combined_socre: {'0': -6.481226492153377, '1': 0.19670041244844388, '2': 0.6, '3': 0.001023951091920664, '4': 0.13333333333333333, '9': 0.29748539497078985, '10': 0.06459807862345093, '13': 0.09170730144332073, '16': -2.7600487796809725, '30': 0.002065212832424003}\n",
      "\n",
      "Step 2\n",
      "query: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #10\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: •Implementation: File metadata indexing (submissio...\n",
      "\n",
      "Search Result #16\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: 3. “Improving Document Retrieval with Contextualiz...\n",
      "\n",
      "Search Result #7\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: vision.”\n",
      "•Expected Results: Notes, slides, or arti...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive list of key use cases across different domains (personal, enterprise, research) is missing.\n",
      "  • - Potential challenges or limitations of using NLP and semantic embeddings for local file search are not thoroughly discussed.\n",
      "  • - Details on how the system would handle different file types or formats are lacking.\n",
      "  • - Information on the specific NLP techniques or models to be used is not provided.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach? How would the system handle different file types and formats, and what specific NLP techniques or models would be used?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current search results provide some relevant information about the problem statement, motivation...\n",
      "combined_socre: {'0': -6.6494265234375, '1': 1.0, '2': 0.5911272727272727, '3': 0.05972656621390348, '7': 0.00048561809329777776, '9': -3.240650317382812, '10': 0.000734438166607892, '16': 0.2930862170087976, '20': 0.08239009222568805}\n",
      "\n",
      "Step 3\n",
      "query: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach? How would the system handle different file types and formats, and what specific NLP techniques or models would be used?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #10\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: •Implementation: File metadata indexing (submissio...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #7\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: vision.”\n",
      "•Expected Results: Notes, slides, or arti...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive list of key use cases across different domains (personal, enterprise, research)\n",
      "  • - Details on potential challenges or limitations beyond specific examples\n",
      "  • - Information on how the system would handle different file types and formats\n",
      "  • - Specific NLP techniques or models that would be used\n",
      "\n",
      "Redundant Content:\n",
      "  • Results  and [Result 4, Result 5] Both provide narrow, specific examples of use cases. Prefer Result 5 as it covers a slightly broader topic. overlap\n",
      "\n",
      "Suggested Refinement: \"What are the key use cases for a local file search engine leveraging natural language processing and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach? How would it handle different file formats? What specific NLP models or techniques could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide some relevant information, such as the problem motivation, proposed appr...\n",
      "combined_socre: {'0': -6.7816293189639145, '1': 0.5, '2': 0.5960494878527666, '32': 0.061010018783517854, '7': 0.0003811975813079875, '9': 0.2955266955266955, '10': 0.0011627761525143758, '11': 0.08245475238330213, '16': -3.253371150909908}\n",
      "\n",
      "Step 4\n",
      "query: \"What are the key use cases for a local file search engine leveraging natural language processing and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach? How would it handle different file formats? What specific NLP models or techniques could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #10\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: •Implementation: File metadata indexing (submissio...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #15\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: Related Work\n",
      "Literature Review:\n",
      "1. “Semantic Deskt...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive list of key use cases across personal, enterprise, and research domains.\n",
      "  • - Potential challenges and limitations beyond specific examples.\n",
      "  • - Details on handling different file formats.\n",
      "  • - Specific NLP models or techniques that could be employed.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What are some key use cases for a local file search engine leveraging NLP and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach beyond specific examples? How would the system handle different file formats? What are some specific NLP models or techniques that could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide a good overview of the problem statement, motivation, and proposed appro...\n",
      "combined_socre: {'0': -6.656753191266741, '1': 0.5, '2': 0.5960494878527666, '7': 0.054128913611306353, '9': 0.2955266955266955, '10': 0.0011627761525143758, '11': 0.08245475238330213, '15': 0.0003811975813079875, '16': -3.19787360297309}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for step in reasoning_steps:\n",
    "    print(f\"Step {count}\")\n",
    "    print(f\"query: {step.query}\")\n",
    "    print(\"results\")\n",
    "    for i in range(len(step.results)):\n",
    "        print(step.results[i])\n",
    "    print(step.reasoning)\n",
    "    print(f\"combined_socre: {step.combined_scores}\\n\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/battalavamshi/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result #1\n",
      "Score: 0.950\n",
      "Source: doc.txt\n",
      "Text: Example text\n",
      "Context: Some context\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.92\n",
      "\n",
      "Relevance Findings:\n",
      "  • result_1: 0.95\n",
      "  • result_2: 0.85\n",
      "\n",
      "Identified Gaps:\n",
      "  • Lack of context in result_1\n",
      "  • Ambiguous phrasing in result_2\n",
      "\n",
      "Redundant Content:\n",
      "  • Results result_1 and result_2 overlap\n",
      "\n",
      "Suggested Refinement: Merge findings for conciseness.\n",
      "\n",
      "Reasoning:\n",
      "  This step evaluates the overlap between results to refine the retrieval strategy for better accuracy...\n"
     ]
    }
   ],
   "source": [
    "from temp import SearchResult, ReasoningStep, SearchIteration\n",
    "import time\n",
    "\n",
    "result = SearchResult(1, \"Example text\", \"Some context\", 0.95, \"/path/to/doc.txt\")\n",
    "print(result)\n",
    "# Output: SearchResult(id=1, score=0.950, source='doc.txt', context: Some context..., text='Example text')\n",
    "example_step = ReasoningStep(\n",
    "    relevance_findings={\"result_1\": 0.95, \"result_2\": 0.85},\n",
    "    gaps_identified=[\"Lack of context in result_1\", \"Ambiguous phrasing in result_2\"],\n",
    "    redundant_content=[(\"result_1\", \"result_2\")],\n",
    "    suggested_refinement=\"Merge findings for conciseness.\",\n",
    "    reasoning_explanation=\"This step evaluates the overlap between results to refine the retrieval strategy for better accuracy.\",\n",
    "    confidence_score=0.92\n",
    ")\n",
    "\n",
    "print(example_step)\n",
    "# Shows structured representation with all components\n",
    "\n",
    "# iteration = SearchIteration(\"query\", [result], reasoning, {\"1\": 0.9}, time.time())\n",
    "# print(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test document\n",
    "test_doc_content = \"\"\"\n",
    "Artificial Intelligence Overview\n",
    "\n",
    "AI is a broad field of computer science focused on creating intelligent machines.\n",
    "Machine learning is a subset of AI that uses data to improve performance.\n",
    "Deep learning is a type of machine learning using neural networks.\n",
    "\"\"\"\n",
    "\n",
    "# Create test directory and document\n",
    "test_dir = Path(\"test_documents\")\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "test_file = test_dir / \"test_article.txt\"\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(test_doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import AsyncDocumentProcessor, BatchConfig\n",
    "from temp import ChainOfThoughtRetriever, SearchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:Selected device for computation: cpu\n",
      "INFO:preprocessing:Using device: cpu\n",
      "INFO:preprocessing:Initializing with 7 processes\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:__main__:Processing test document...\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor for document processing\n",
    "processor = AsyncDocumentProcessor(\n",
    "    embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    anthropic_api_key=api_key,\n",
    "    device='cpu',  # Using CPU for testing\n",
    "    batch_config=BatchConfig(\n",
    "        embeddings=32,\n",
    "        context=10,\n",
    "        faiss=1000,\n",
    "        documents=5,\n",
    "        process=4\n",
    "    ),\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=2\n",
    ")\n",
    "test_dir = Path(\"processed_documents\")\n",
    "# Process test document\n",
    "logger.info(\"Processing test document...\")\n",
    "processed_docs = await processor.load_indices(str(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: None\n",
      "Processed documents: {'/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt': {'metadata': {'file_path': '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt', 'file_name': 'test_text.txt', 'file_type': '.txt', 'created_time': '2024-11-24T11:57:27.521049', 'modified_time': '2024-11-24T11:57:27.521049', 'size_bytes': 475, 'num_chunks': 1, 'processing_time': 5.821021, 'batch_sizes': {'embeddings': 32, 'context': 10, 'faiss': 1000, 'documents': 5, 'process': 2}}, 'chunks': [{'chunk_id': 0, 'text': '\"\"\"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the English alphabet, making it a popular pangram. It\\'s often used for typing practice, font displays, and testing equipment. While simple, this sentence serves as a great tool for showcasing how all the letters are used in different contexts.\\n    It’s a fun and quirky way to test a variety of systems and applications that require the use of all characters in the English alphabet.\"\"\"', 'start_char': 0, 'end_char': 473, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'sentence', 'contains', 'every', 'letter', 'english', 'alphabet', 'making', 'popular', 'pangram', 'often', 'used', 'typing', 'practice', 'font', 'displays', 'testing', 'equipment', 'simple', 'sentence', 'serves', 'great', 'tool', 'showcasing', 'letters', 'used', 'different', 'contexts', 'fun', 'quirky', 'way', 'test', 'variety', 'systems', 'applications', 'require', 'use', 'characters', 'english', 'alphabet']}], 'faiss_index': <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x16505ac30> >, 'bm25_index': <rank_bm25.BM25Okapi object at 0x16505aa30>}, '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf': {'metadata': {'file_path': '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf', 'file_name': '1.pdf', 'file_type': '.pdf', 'created_time': '2024-11-24T09:53:19.406365', 'modified_time': '2024-10-06T02:48:24.092947', 'size_bytes': 89986, 'num_chunks': 23, 'processing_time': 23.03453, 'batch_sizes': {'embeddings': 32, 'context': 10, 'faiss': 1000, 'documents': 5, 'process': 2}}, 'chunks': [{'chunk_id': 0, 'text': 'Efficient Local File Search Engine Using Large Language Models\\nProblem Statement\\nContext: With the exponential growth of digital information, individuals and organizations store vast\\namounts of data locally on personal devices and enterprise servers. Traditional file search tools rely heavily\\non exact keyword matching, lacking the ability to understand the context or semantics behind user queries', 'start_char': 0, 'end_char': 399, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['efficient', 'local', 'file', 'search', 'engine', 'using', 'large', 'language', 'models', 'problem', 'statement', 'context', 'exponential', 'growth', 'digital', 'information', 'individuals', 'organizations', 'store', 'vast', 'amounts', 'data', 'locally', 'personal', 'devices', 'enterprise', 'servers', 'traditional', 'file', 'search', 'tools', 'rely', 'heavily', 'exact', 'keyword', 'matching', 'lacking', 'ability', 'understand', 'context', 'semantics', 'behind', 'user', 'queries']}, {'chunk_id': 1, 'text': 'and document content. This limitation often leads to inefficient retrieval processes, where users struggle to\\nlocate relevant files amidst overwhelming data volumes.\\nProblem: There is a significant gap in the availability of local search engines that can interpret natural\\nlanguage queries and retrieve files based on semantic relevance rather than mere keyword occurrence. Ex-\\nisting solutions do not effectively leverage advancements in Large Language Models (LLMs) for local data', 'start_char': 400, 'end_char': 882, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['document', 'content', 'limitation', 'often', 'leads', 'inefficient', 'retrieval', 'processes', 'users', 'struggle', 'locate', 'relevant', 'files', 'amidst', 'overwhelming', 'data', 'volumes', 'problem', 'significant', 'gap', 'availability', 'local', 'search', 'engines', 'interpret', 'natural', 'language', 'queries', 'retrieve', 'files', 'based', 'semantic', 'relevance', 'rather', 'mere', 'keyword', 'occurrence', 'isting', 'solutions', 'effectively', 'leverage', 'advancements', 'large', 'language', 'models', 'llms', 'local', 'data']}, {'chunk_id': 2, 'text': 'environments. Our project aims to address this challenge by developing a novel local search engine that\\nharnesses the power of LLMs to enable efficient, context-aware retrieval of files, enhancing productivity and\\ndata accessibility.\\nProject Objectives\\n•Design and develop an innovative local file search engine that interprets natural language queries using\\nLLMs.\\n•Implement a semantic indexing mechanism that transforms local files into meaningful embeddings for\\neffective retrieval.', 'start_char': 883, 'end_char': 1368, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['environments', 'project', 'aims', 'address', 'challenge', 'developing', 'novel', 'local', 'search', 'engine', 'harnesses', 'power', 'llms', 'enable', 'efficient', 'retrieval', 'files', 'enhancing', 'productivity', 'data', 'accessibility', 'project', 'objectives', 'develop', 'innovative', 'local', 'file', 'search', 'engine', 'interprets', 'natural', 'language', 'queries', 'using', 'llms', 'semantic', 'indexing', 'mechanism', 'transforms', 'local', 'files', 'meaningful', 'embeddings', 'effective', 'retrieval']}, {'chunk_id': 3, 'text': 'effective retrieval.\\n•Create a simple user interface facilitating interaction with the search system.\\n•Evaluate and optimize the search engine’s performance using rigorous metrics to ensure high accuracy\\nand efficiency.\\nUse Cases\\n1. Contextual Retrieval of Class Notes\\nQuery: ”What were the main topics covered in last week’s Machine Learning lecture?”\\n•Expected Results: Class notes or lecture slides from the most recent Machine Learning lecture.', 'start_char': -1, 'end_char': 447, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['effective', 'retrieval', 'simple', 'user', 'interface', 'facilitating', 'interaction', 'search', 'system', 'optimize', 'search', 'engine', 'performance', 'using', 'rigorous', 'metrics', 'ensure', 'high', 'accuracy', 'efficiency', 'use', 'cases', 'contextual', 'retrieval', 'class', 'notes', 'query', 'main', 'topics', 'covered', 'last', 'week', 'machine', 'learning', 'lecture', 'results', 'class', 'notes', 'lecture', 'slides', 'recent', 'machine', 'learning', 'lecture']}, {'chunk_id': 4, 'text': '•Challenges: Temporal understanding (finding the correct week’s material), identifying important\\ntopics, filtering irrelevant notes.\\n•Implementation: Date-based filtering of notes, key topic extraction from lecture content.\\n2. Semantic Search for Assignment Guidelines\\nQuery: ”Find the guidelines for the upcoming Operating Systems project.”\\n•Expected Results: Documents or web links containing project guidelines or assignment descriptions.', 'start_char': 1797, 'end_char': 2238, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['temporal', 'understanding', 'finding', 'correct', 'week', 'material', 'identifying', 'important', 'topics', 'filtering', 'irrelevant', 'notes', 'filtering', 'notes', 'key', 'topic', 'extraction', 'lecture', 'content', 'semantic', 'search', 'assignment', 'guidelines', 'query', 'find', 'guidelines', 'upcoming', 'operating', 'systems', 'results', 'documents', 'web', 'links', 'containing', 'project', 'guidelines', 'assignment', 'descriptions']}, {'chunk_id': 5, 'text': '•Challenges: Differentiating between similar projects, recognizing assignment-specific terms.\\n1 •Implementation: Natural language processing for identifying assignment names, course-specific key-\\nword mapping, version control to find the latest guidelines.\\n3. Cross-Document Study Material Discovery\\nQuery: ”Show me all notes related to convolutional neural networks and their applications in computer\\nvision.”', 'start_char': 2239, 'end_char': 2649, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['differentiating', 'similar', 'projects', 'recognizing', 'terms', 'natural', 'language', 'processing', 'identifying', 'assignment', 'names', 'word', 'mapping', 'version', 'control', 'find', 'latest', 'guidelines', 'study', 'material', 'discovery', 'query', 'show', 'notes', 'related', 'convolutional', 'neural', 'networks', 'applications', 'computer', 'vision']}, {'chunk_id': 6, 'text': 'vision.”\\n•Expected Results: Notes, slides, or articles covering convolutional neural networks (CNNs) and\\ncomputer vision examples from various classes or study materials.\\n•Challenges: Linking concepts across different documents and subjects, handling related terms like\\n‘CNN’ and ‘image recognition.’\\n•Implementation: Entity recognition for key terms (CNN, computer vision), cross-document linking,\\ntopic modeling to identify relevant content.\\n4. Search for Exam Policies', 'start_char': -1, 'end_char': 470, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['results', 'notes', 'slides', 'articles', 'covering', 'convolutional', 'neural', 'networks', 'cnns', 'computer', 'vision', 'examples', 'various', 'classes', 'study', 'materials', 'linking', 'concepts', 'across', 'different', 'documents', 'subjects', 'handling', 'related', 'terms', 'like', 'cnn', 'image', 'entity', 'recognition', 'key', 'terms', 'cnn', 'computer', 'vision', 'linking', 'topic', 'modeling', 'identify', 'relevant', 'content', 'search', 'exam', 'policies']}, {'chunk_id': 7, 'text': '4. Search for Exam Policies\\nQuery: ”What are the university’s rules on retaking exams?”\\n•Expected Results: University or department policy documents that explain the rules for retaking\\nexams or handling missed exams.\\n•Challenges: Understanding different policy terminologies, finding up-to-date documents, differenti-\\nating between departments.\\n•Implementation: Policy document segmentation, time-based document retrieval (to find the latest', 'start_char': 3085, 'end_char': 3526, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['search', 'exam', 'policies', 'query', 'university', 'rules', 'retaking', 'exams', 'results', 'university', 'department', 'policy', 'documents', 'explain', 'rules', 'retaking', 'exams', 'handling', 'missed', 'exams', 'understanding', 'different', 'policy', 'terminologies', 'finding', 'documents', 'ating', 'departments', 'policy', 'document', 'segmentation', 'document', 'retrieval', 'find', 'latest']}, {'chunk_id': 8, 'text': 'rules), synonym handling (e.g., ’retake,’ ’make-up exam’).\\n5. Personalized Search for Assignment Submissions\\nQuery: ”Retrieve all the reports I submitted for the Data Structures course last semester.”\\n•Expected Results: Submitted reports or project files for the Data Structures course from the previous\\nsemester.\\n•Challenges: Time frame filtering, identifying the correct course, user-specific data filtering.', 'start_char': 3527, 'end_char': 3937, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['rules', 'synonym', 'handling', 'retake', 'exam', 'personalized', 'search', 'assignment', 'submissions', 'query', 'retrieve', 'reports', 'submitted', 'data', 'structures', 'course', 'last', 'results', 'submitted', 'reports', 'project', 'files', 'data', 'structures', 'course', 'previous', 'semester', 'time', 'frame', 'filtering', 'identifying', 'correct', 'course', 'data', 'filtering']}, {'chunk_id': 9, 'text': '•Implementation: File metadata indexing (submission dates, course tags), project tagging, user-\\nspecific search.\\nMethodology\\nApproach: We propose to build a novel semantic search system tailored for local file environments, lever-\\naging LLMs to understand and process natural language queries. This system will bridge the gap between\\nadvanced language understanding models and local data search, a relatively unexplored domain.', 'start_char': 3938, 'end_char': 4365, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['file', 'metadata', 'indexing', 'submission', 'dates', 'course', 'tags', 'project', 'tagging', 'specific', 'search', 'methodology', 'approach', 'propose', 'build', 'novel', 'semantic', 'search', 'system', 'tailored', 'local', 'file', 'environments', 'aging', 'llms', 'understand', 'process', 'natural', 'language', 'queries', 'system', 'bridge', 'gap', 'advanced', 'language', 'understanding', 'models', 'local', 'data', 'search', 'relatively', 'unexplored', 'domain']}, {'chunk_id': 10, 'text': 'LLM(s) and Techniques: The project will utilize state-of-the-art open-source LLMs such as OpenAI’s\\nGPT-4 or the latest NVIDIA’s NV-embed for generating text embeddings. Techniques like Sentence Trans-\\nformers will be used to create embeddings that capture the semantic essence of documents. Libraries such as\\nHugging Face Transformers and FAISS/Milvus will be employed for efficient indexing and similarity search\\noperations.\\nArchitecture/Process:', 'start_char': 4366, 'end_char': 4813, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['llm', 'techniques', 'project', 'utilize', 'llms', 'openai', 'latest', 'nvidia', 'generating', 'text', 'embeddings', 'techniques', 'like', 'sentence', 'formers', 'used', 'create', 'embeddings', 'capture', 'semantic', 'essence', 'documents', 'libraries', 'hugging', 'face', 'transformers', 'employed', 'efficient', 'indexing', 'similarity', 'search', 'operations']}, {'chunk_id': 11, 'text': 'operations.\\nArchitecture/Process:\\n1. Data Ingestion and Preprocessing: Collect and parse local files in various formats (e.g., .txt, .pdf,\\n.docx). Preprocess the text to clean and normalize content, handling encoding issues and extracting\\nmeaningful text.\\n2 2. Semantic Embedding Generation: Use LLMs to convert preprocessed text into high-dimensional se-\\nmantic embeddings that reflect the contextual meaning of documents.', 'start_char': -1, 'end_char': 422, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['operations', 'data', 'ingestion', 'preprocessing', 'collect', 'parse', 'local', 'files', 'various', 'formats', 'preprocess', 'text', 'clean', 'normalize', 'content', 'handling', 'encoding', 'issues', 'extracting', 'meaningful', 'text', 'semantic', 'embedding', 'generation', 'use', 'llms', 'convert', 'preprocessed', 'text', 'mantic', 'embeddings', 'reflect', 'contextual', 'meaning', 'documents']}, {'chunk_id': 12, 'text': '3. Indexing: Store these embeddings in a vector database, enabling efficient similarity searches through\\napproximate nearest neighbor algorithms.\\n4. Natural Language Query Processing: Accept user queries in plain language, generate query embeddings\\nusing the same LLM to ensure consistency in semantic space.\\n5. Semantic Search and Ranking: Perform similarity searches to find embeddings closest to the query\\nembedding and rank the results based on relevance scores.', 'start_char': 5204, 'end_char': 5670, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['indexing', 'store', 'embeddings', 'vector', 'database', 'enabling', 'efficient', 'similarity', 'searches', 'approximate', 'nearest', 'neighbor', 'algorithms', 'natural', 'language', 'query', 'processing', 'accept', 'user', 'queries', 'plain', 'language', 'generate', 'query', 'embeddings', 'using', 'llm', 'ensure', 'consistency', 'semantic', 'space', 'semantic', 'search', 'ranking', 'perform', 'similarity', 'searches', 'find', 'embeddings', 'closest', 'query', 'embedding', 'rank', 'results', 'based', 'relevance', 'scores']}, {'chunk_id': 13, 'text': '6. User Interface and Results Presentation: Develop an interface that displays search results with con-\\ntextually relevant snippets, allowing users to preview content.\\nData: The data comprises the user’s local files, ensuring privacy and security as all processing occurs locally.\\nNo external data transmission is involved.\\nEvaluation: Success will be measured using evaluation metrics like Mean Reciprocal Rank (MRR), Preci-\\nsion@K, and user satisfaction scores.\\nRelated Work\\nLiterature Review:', 'start_char': 5671, 'end_char': 6166, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['user', 'interface', 'results', 'presentation', 'develop', 'interface', 'displays', 'search', 'results', 'textually', 'relevant', 'snippets', 'allowing', 'users', 'preview', 'content', 'data', 'data', 'comprises', 'user', 'local', 'files', 'ensuring', 'privacy', 'security', 'processing', 'occurs', 'locally', 'external', 'data', 'transmission', 'involved', 'evaluation', 'success', 'measured', 'using', 'evaluation', 'metrics', 'like', 'mean', 'reciprocal', 'rank', 'mrr', 'sion', 'k', 'user', 'satisfaction', 'scores', 'related', 'work', 'literature', 'review']}, {'chunk_id': 14, 'text': 'Related Work\\nLiterature Review:\\n1. “Semantic Desktop Search: A New Paradigm” by Smith et al. (2020) discusses applying semantic\\nsearch to desktop environments using ontology-based methods, highlighting limitations in handling\\nunstructured data.\\n2. “Leveraging Transformer Models for Information Retrieval” by Lee and Chen (2021) explores transformer-\\nbased models in web search engines but does not focus on local file systems or privacy concerns.', 'start_char': -1, 'end_char': 446, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['related', 'work', 'literature', 'review', 'semantic', 'desktop', 'search', 'new', 'paradigm', 'smith', 'et', 'al', 'discusses', 'applying', 'semantic', 'search', 'desktop', 'environments', 'using', 'methods', 'highlighting', 'limitations', 'handling', 'unstructured', 'data', 'leveraging', 'transformer', 'models', 'information', 'retrieval', 'lee', 'chen', 'explores', 'based', 'models', 'web', 'search', 'engines', 'focus', 'local', 'file', 'systems', 'privacy', 'concerns']}, {'chunk_id': 15, 'text': '3. “Improving Document Retrieval with Contextualized Language Representations” by Zhang and Liu\\n(2019) investigates contextual embeddings for document retrieval in cloud services, lacking application\\nto local environments.\\nPositioning: Our project differentiates itself by being among the first to integrate advanced LLMs into a\\nlocal file search engine, providing a novel application of these models in a domain that has not fully utilized', 'start_char': 6583, 'end_char': 7023, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['improving', 'document', 'retrieval', 'contextualized', 'language', 'representations', 'zhang', 'liu', 'investigates', 'contextual', 'embeddings', 'document', 'retrieval', 'cloud', 'services', 'lacking', 'application', 'local', 'environments', 'positioning', 'project', 'differentiates', 'among', 'first', 'integrate', 'advanced', 'llms', 'local', 'file', 'search', 'engine', 'providing', 'novel', 'application', 'models', 'domain', 'fully', 'utilized']}, {'chunk_id': 16, 'text': 'their capabilities. We aim to fill the gap identified in existing literature by bringing semantic understanding\\nto personal and organizational file search, enhancing the efficiency and accuracy of information retrieval in\\nlocal environments.\\nTimeline\\nPhase 1 (Weeks 1–4):\\n1. Week 1: Finalize project requirements, select appropriate LLMs, and design system architecture.\\n2. Week 2: Develop data ingestion and preprocessing pipelines; initiate embedding generation for a sample\\ndataset.', 'start_char': 7024, 'end_char': 7509, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['capabilities', 'aim', 'fill', 'gap', 'identified', 'existing', 'literature', 'bringing', 'semantic', 'understanding', 'personal', 'organizational', 'file', 'search', 'enhancing', 'efficiency', 'accuracy', 'information', 'retrieval', 'local', 'environments', 'timeline', 'phase', 'weeks', 'week', 'finalize', 'project', 'requirements', 'select', 'appropriate', 'llms', 'design', 'system', 'architecture', 'week', 'develop', 'data', 'ingestion', 'preprocessing', 'pipelines', 'initiate', 'embedding', 'generation', 'sample', 'dataset']}, {'chunk_id': 17, 'text': 'dataset.\\n3. Week 3: Complete embedding generation for all target files; implement the vector database indexing.\\n4. Week 4: Develop basic query processing and search functionalities; conduct initial testing.\\nPhase 2 (Weeks 5–8):\\n1. Week 5: Enhance query processing to handle complex queries; optimize embedding generation.\\n2. Week 6: Design and implement the user interface; integrate frontend and backend components.\\n3 3. Week 7: Conduct performance optimization; implement feedback mechanisms.', 'start_char': -1, 'end_char': 493, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['dataset', 'week', 'complete', 'embedding', 'generation', 'target', 'files', 'implement', 'vector', 'database', 'indexing', 'week', 'develop', 'basic', 'query', 'processing', 'search', 'functionalities', 'conduct', 'initial', 'testing', 'phase', 'weeks', 'week', 'enhance', 'query', 'processing', 'handle', 'complex', 'queries', 'optimize', 'embedding', 'generation', 'week', 'design', 'implement', 'user', 'interface', 'integrate', 'frontend', 'backend', 'components', 'week', 'conduct', 'performance', 'optimization', 'implement', 'feedback', 'mechanisms']}, {'chunk_id': 18, 'text': '4. Week 8: Perform comprehensive testing and evaluation; prepare final report and presentation materials.\\nMilestones:\\n1. Week 2: Completion of data preprocessing and initial embedding generation.\\n2. Week 4: Operational prototype of the search engine with basic functionalities.\\n3. Week 6: User interface completed and integrated with the backend.\\n4. Week 8: Final system evaluation and submission of deliverables.\\nChallenges and Risks', 'start_char': 7996, 'end_char': 8430, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['week', 'perform', 'comprehensive', 'testing', 'evaluation', 'prepare', 'final', 'report', 'presentation', 'materials', 'milestones', 'week', 'completion', 'data', 'preprocessing', 'initial', 'embedding', 'generation', 'week', 'operational', 'prototype', 'search', 'engine', 'basic', 'functionalities', 'week', 'user', 'interface', 'completed', 'integrated', 'backend', 'week', 'final', 'system', 'evaluation', 'submission', 'deliverables', 'challenges', 'risks']}, {'chunk_id': 19, 'text': 'Challenges and Risks\\n•Computational Limitations: Generating embeddings for numerous files may strain resources. Mit-\\nigation: Optimize code efficiency, use mini-batches, and limit initial scope if necessary.\\n•Time Constraints: Developing a fully featured system in two months is challenging. Mitigation:\\nPrioritize essential features; employ agile methodologies for rapid development cycles.\\n•Model Adaptation: Pre-trained LLMs may not perfectly align with domain-specific language in files.', 'start_char': -1, 'end_char': 490, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['challenges', 'risks', 'limitations', 'generating', 'embeddings', 'numerous', 'files', 'may', 'strain', 'resources', 'igation', 'optimize', 'code', 'efficiency', 'use', 'limit', 'initial', 'scope', 'necessary', 'constraints', 'developing', 'fully', 'featured', 'system', 'two', 'months', 'challenging', 'mitigation', 'prioritize', 'essential', 'features', 'employ', 'agile', 'methodologies', 'rapid', 'development', 'cycles', 'adaptation', 'llms', 'may', 'perfectly', 'align', 'language', 'files']}, {'chunk_id': 20, 'text': 'Mitigation: Implement domain adaptation techniques or fine-tune models if time permits.\\n•Privacy Concerns: Users may be cautious about processing sensitive data. Mitigation: Ensure all\\noperations are confined to the local environment with no external data transfers.\\nResources Needed\\nHardware/Software:\\n•A development machine with at least 16 GB RAM; a dedicated GPU is advantageous.\\n•Python environment with libraries: Transformers, FAISS/Milvus, PyTorch or TensorFlow.\\nData Requirements:', 'start_char': 8902, 'end_char': 9391, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['mitigation', 'implement', 'domain', 'adaptation', 'techniques', 'models', 'time', 'permits', 'concerns', 'users', 'may', 'cautious', 'processing', 'sensitive', 'data', 'mitigation', 'ensure', 'operations', 'confined', 'local', 'environment', 'external', 'data', 'transfers', 'resources', 'needed', 'development', 'machine', 'least', 'gb', 'ram', 'dedicated', 'gpu', 'advantageous', 'environment', 'libraries', 'transformers', 'pytorch', 'tensorflow', 'data', 'requirements']}, {'chunk_id': 21, 'text': 'Data Requirements:\\n•Access to a diverse set of local files for testing (e.g., documents, reports).\\n•Pre-trained LLMs accessible locally without relying on external APIs.\\nExpected Deliverables\\n•Code:\\n–Modular and well-documented scripts for data preprocessing, embedding generation, indexing,\\nquery processing, and search.\\n–A user interface application for interacting with the search engine.\\n–Deployment instructions and a user guide.\\n•Final Report:', 'start_char': -1, 'end_char': 448, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['data', 'requirements', 'diverse', 'set', 'local', 'files', 'testing', 'documents', 'reports', 'llms', 'accessible', 'locally', 'without', 'relying', 'external', 'apis', 'expected', 'deliverables', 'scripts', 'data', 'preprocessing', 'embedding', 'generation', 'indexing', 'query', 'processing', 'search', 'user', 'interface', 'application', 'interacting', 'search', 'engine', 'instructions', 'user', 'guide', 'report']}, {'chunk_id': 22, 'text': '•Final Report:\\n–Introduction outlining project goals and significance.\\n–Detailed methodology explaining technical implementations and architectural decisions.\\n–Experimental results with evaluation metrics and analysis.\\n–Discussion of challenges faced, solutions implemented, and potential future enhancements.\\n–Conclusion summarizing findings and contributions.\\n4', 'start_char': 9808, 'end_char': 10171, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['report', 'outlining', 'project', 'goals', 'significance', 'methodology', 'explaining', 'technical', 'implementations', 'architectural', 'decisions', 'results', 'evaluation', 'metrics', 'analysis', 'challenges', 'faced', 'solutions', 'implemented', 'potential', 'future', 'enhancements', 'summarizing', 'findings', 'contributions']}], 'faiss_index': <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x168463420> >, 'bm25_index': <rank_bm25.BM25Okapi object at 0x1684855e0>}, '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf': {'metadata': {'file_path': '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf', 'file_name': '2.pdf', 'file_type': '.pdf', 'created_time': '2024-11-24T09:53:23.037822', 'modified_time': '2023-07-01T11:29:13.937205', 'size_bytes': 31250, 'num_chunks': 15, 'processing_time': 29.406026, 'batch_sizes': {'embeddings': 32, 'context': 10, 'faiss': 1000, 'documents': 5, 'process': 2}}, 'chunks': [{'chunk_id': 0, 'text': 'STATEMENT OF PURPOSE\\nBattala Vamshi Krishna\\n”Computer science empowers students to create the world of tomorrow.”\\n- Satya Nadella, CEO of Microsoft\\nI got glimpses of the brilliant world of computers at a very early stage when I stumbled upon the\\nremains of a computer lying in my house’s storage area. I vividly remember collecting it and trying\\nto make sense out of it. Despite being unable to accomplish much, my curiosity did not stop me.', 'start_char': 0, 'end_char': 441, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['statement', 'purpose', 'battala', 'vamshi', 'krishna', 'computer', 'science', 'empowers', 'students', 'create', 'world', 'satya', 'nadella', 'ceo', 'microsoft', 'got', 'glimpses', 'brilliant', 'world', 'computers', 'early', 'stage', 'stumbled', 'upon', 'remains', 'computer', 'lying', 'house', 'storage', 'area', 'vividly', 'remember', 'collecting', 'trying', 'make', 'sense', 'despite', 'unable', 'accomplish', 'much', 'curiosity', 'stop']}, {'chunk_id': 1, 'text': 'Throughout my formative years, I immersed myself in beginner-level books and online resources,\\navidly learning about computers and technology, realizing that I needed to take my knowledge to\\nthe next level.\\nMy journey with computers brought me to IIT Kharagpur, one of India’s most prestigious in-\\nstitutes. Enrolling in their Bachelor of Technology program in Computer Science marked a sig-\\nnificant turning point in my knowledge pursuit. While at IIT Kharagpur, I was immersed in a', 'start_char': 442, 'end_char': 925, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['throughout', 'formative', 'years', 'immersed', 'books', 'online', 'resources', 'avidly', 'learning', 'computers', 'technology', 'realizing', 'needed', 'take', 'knowledge', 'next', 'level', 'journey', 'computers', 'brought', 'iit', 'kharagpur', 'one', 'india', 'prestigious', 'stitutes', 'enrolling', 'bachelor', 'technology', 'program', 'computer', 'science', 'marked', 'nificant', 'turning', 'point', 'knowledge', 'pursuit', 'iit', 'kharagpur', 'immersed']}, {'chunk_id': 2, 'text': 'comprehensive curriculum covering various computer science subjects. From fundamental courses\\nin programming & data structures, and algorithms to advanced topics such as operating systems,\\ndatabases, and Natural Language Processing, I have achieved a high level of understanding of fun-\\ndamental principles in computer science. Over my initial semesters, I gradually enhanced my pro-\\ngramming abilities and sharpened my skills in analyzing algorithms. As I progressed in my studies,', 'start_char': 926, 'end_char': 1408, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['comprehensive', 'curriculum', 'covering', 'various', 'computer', 'science', 'subjects', 'fundamental', 'courses', 'programming', 'data', 'structures', 'algorithms', 'advanced', 'topics', 'operating', 'systems', 'databases', 'natural', 'language', 'processing', 'achieved', 'high', 'level', 'understanding', 'damental', 'principles', 'computer', 'science', 'initial', 'semesters', 'gradually', 'enhanced', 'gramming', 'abilities', 'sharpened', 'skills', 'analyzing', 'algorithms', 'progressed', 'studies']}, {'chunk_id': 3, 'text': 'I had the opportunity to explore advanced topics in computer science.\\nI got my first taste of lower-level programming in my fifth semester when I took the computer\\norganization & architecture course, instructed by Professor Bhargab Bikram Bhattacharya. This\\ncourse allowed me to delve deep into the principles of computer design & intricacies of proces-\\nsor functionality. Drawing on this knowledge, I developed a single-cycle processor, meticulously', 'start_char': 1409, 'end_char': 1859, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['opportunity', 'explore', 'advanced', 'topics', 'computer', 'science', 'got', 'first', 'taste', 'programming', 'fifth', 'semester', 'took', 'computer', 'organization', 'architecture', 'course', 'instructed', 'professor', 'bhargab', 'bikram', 'bhattacharya', 'course', 'allowed', 'delve', 'deep', 'principles', 'computer', 'design', 'intricacies', 'sor', 'functionality', 'drawing', 'knowledge', 'developed', 'processor', 'meticulously']}, {'chunk_id': 4, 'text': 'crafting the datapath and control signals to ensure seamless instruction execution. Synthesizing the\\nVerilog code on the Xilinx ISE validated my design and gave me valuable insights into the hardware\\nimplementation of a processor.\\nDuring the subsequent semester, I immersed myself in the systems aspect of computers through\\ncourses like Operating Systems (OS) and Computer Networks. In the OS course, I understood', 'start_char': 1860, 'end_char': 2273, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['crafting', 'datapath', 'control', 'signals', 'ensure', 'seamless', 'instruction', 'execution', 'synthesizing', 'verilog', 'code', 'xilinx', 'ise', 'validated', 'design', 'gave', 'valuable', 'insights', 'hardware', 'implementation', 'processor', 'subsequent', 'semester', 'immersed', 'systems', 'aspect', 'computers', 'courses', 'like', 'operating', 'systems', 'os', 'computer', 'networks', 'os', 'course', 'understood']}, {'chunk_id': 5, 'text': 'many concepts, encompassing everything from syscalls to memory management and filesystems.\\nThis knowledge enabled me to undertake exciting projects, including developing shells, working\\nwith shared memory and threads, and creating memory management systems. The computer net-\\nworks course taught me about the interconnection and communication aspects between multiple\\ncomputers. These included concepts ranging from basic medium-sharing protocols to advanced', 'start_char': 2274, 'end_char': 2732, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['many', 'concepts', 'encompassing', 'everything', 'syscalls', 'memory', 'management', 'filesystems', 'knowledge', 'enabled', 'undertake', 'exciting', 'projects', 'including', 'developing', 'shells', 'working', 'shared', 'memory', 'threads', 'creating', 'memory', 'management', 'systems', 'computer', 'works', 'course', 'taught', 'interconnection', 'communication', 'aspects', 'multiple', 'computers', 'included', 'concepts', 'ranging', 'basic', 'protocols', 'advanced']}, {'chunk_id': 6, 'text': 'TCP/IP configurations. Building upon this background, I did some creative projects, including\\nconstructing my customized Linux traceroute and creating a static library that uses a reliable pro-\\ntocol over UDP Sockets.\\nFurthermore, I explored the vast fields of computer science. I got a glimpse into data science\\nwhen I took artificial intelligence, machine learning natural language processing. The computer', 'start_char': 2733, 'end_char': 3141, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['configurations', 'building', 'upon', 'background', 'creative', 'projects', 'including', 'constructing', 'customized', 'linux', 'traceroute', 'creating', 'static', 'library', 'uses', 'reliable', 'tocol', 'udp', 'sockets', 'furthermore', 'explored', 'vast', 'fields', 'computer', 'science', 'got', 'glimpse', 'data', 'science', 'took', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'computer']}, {'chunk_id': 7, 'text': 'graphics course introduced me to the fascinating world of graphics and visualization. I dived into\\n1 the theoretical aspects of computer science by studying courses on formal languages, automata\\ntheory, and the theory of computation. Additionally, I gained insights into the data aspects of\\ncomputer science through courses on database management systems and big data processing.\\nI also had a chance to collaborate with incredible professors, such as working with Professor', 'start_char': 3142, 'end_char': 3615, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['graphics', 'course', 'introduced', 'fascinating', 'world', 'graphics', 'visualization', 'dived', 'theoretical', 'aspects', 'computer', 'science', 'studying', 'courses', 'formal', 'languages', 'automata', 'theory', 'theory', 'computation', 'additionally', 'gained', 'insights', 'data', 'aspects', 'computer', 'science', 'courses', 'database', 'management', 'systems', 'big', 'data', 'processing', 'also', 'chance', 'collaborate', 'incredible', 'professors', 'working', 'professor']}, {'chunk_id': 8, 'text': 'K.S. Rao, concerning my Bachelor’s project for a semester. The project was titled ”Developing an\\nEnd-to-End System for Accent Classification from Emotional Speech,” with particular emphasis\\non raw sound waveforms. By directly working with the unprocessed waveforms, we could capture\\nsubtle nuances and characteristics often overlooked in traditional spectral-based techniques. We\\nexplored different feature extraction methods, conducted thorough testing, and identified the opti-', 'start_char': 3616, 'end_char': 4095, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['rao', 'concerning', 'bachelor', 'project', 'semester', 'project', 'titled', 'developing', 'system', 'accent', 'classification', 'emotional', 'speech', 'particular', 'emphasis', 'raw', 'sound', 'waveforms', 'directly', 'working', 'unprocessed', 'waveforms', 'could', 'capture', 'subtle', 'nuances', 'characteristics', 'often', 'overlooked', 'traditional', 'techniques', 'explored', 'different', 'feature', 'extraction', 'methods', 'conducted', 'thorough', 'testing', 'identified']}, {'chunk_id': 9, 'text': 'mal approach. Our goal was accurate accent classification and gaining insights from the underlying\\nmechanisms of raw waveforms. Working closely with Professor K.S. Rao throughout the project\\nwas a privilege. His expertise and guidance significantly enchanced my understanding of speech\\nanalysis and the broader field of computer science.\\nContinuing my journey in computer science, I was driven to deepen my understanding and', 'start_char': 4096, 'end_char': 4520, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['mal', 'approach', 'goal', 'accurate', 'accent', 'classification', 'gaining', 'insights', 'underlying', 'mechanisms', 'raw', 'waveforms', 'working', 'closely', 'professor', 'rao', 'throughout', 'project', 'privilege', 'expertise', 'guidance', 'significantly', 'enchanced', 'understanding', 'speech', 'analysis', 'broader', 'field', 'computer', 'science', 'continuing', 'journey', 'computer', 'science', 'driven', 'deepen', 'understanding']}, {'chunk_id': 10, 'text': 'bridge the gap between academia and industry, and I actively pursued opportunities to collaborate\\nwith professionals in the field. In pursuit of this goal, I applied for an internship at Nvidia, where I\\nhad the chance to explore the realm of system-level designing and embedded systems. During my\\ninternship, I thoroughly studied the register specifications manuals of the Nvidia Orin System on\\nChip (SOC) and devised a programming sequence based on the acquired knowledge. Subsequently,', 'start_char': 4521, 'end_char': 5008, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['bridge', 'gap', 'academia', 'industry', 'actively', 'pursued', 'opportunities', 'collaborate', 'professionals', 'field', 'pursuit', 'goal', 'applied', 'internship', 'nvidia', 'chance', 'explore', 'realm', 'designing', 'embedded', 'systems', 'internship', 'thoroughly', 'studied', 'register', 'specifications', 'manuals', 'nvidia', 'orin', 'system', 'chip', 'soc', 'devised', 'programming', 'sequence', 'based', 'acquired', 'knowledge', 'subsequently']}, {'chunk_id': 11, 'text': 'I used it to design and implement a QNX resource manager for the TKE Watchdog timer on Orin\\nSOC. I created a client daemon to validate the basic functionalities, such as start, stop and ping.\\nWe had to change the device tree and rebuild the entire database to test additional safety modes,\\nsuch as windowed and challenge-response modes. I have created parameters to access the device\\ntree to test it efficiently. Working with a team of eight highly qualified developers with extensive', 'start_char': 5009, 'end_char': 5493, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['used', 'design', 'implement', 'qnx', 'resource', 'manager', 'tke', 'watchdog', 'timer', 'orin', 'soc', 'created', 'client', 'daemon', 'validate', 'basic', 'functionalities', 'start', 'stop', 'ping', 'change', 'device', 'tree', 'rebuild', 'entire', 'database', 'test', 'additional', 'safety', 'modes', 'windowed', 'modes', 'created', 'parameters', 'access', 'device', 'tree', 'test', 'efficiently', 'working', 'team', 'eight', 'highly', 'qualified', 'developers', 'extensive']}, {'chunk_id': 12, 'text': 'collaboration helped me improve my communication and teamwork skills.\\nAfter delving into the vast and diverse fields of computer science and collaborating with highly\\nknowledgeable individuals, my passion for the subject has driven me to pursue further exploration\\nand mastery. While standing at a crossroads and deciding my future, I became aware of the ex-\\nceptional academic and research opportunities in computer science offered at [..XX..] University.', 'start_char': 5494, 'end_char': 5950, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['collaboration', 'helped', 'improve', 'communication', 'teamwork', 'skills', 'delving', 'vast', 'diverse', 'fields', 'computer', 'science', 'collaborating', 'highly', 'knowledgeable', 'individuals', 'passion', 'subject', 'driven', 'pursue', 'exploration', 'mastery', 'standing', 'crossroads', 'deciding', 'future', 'became', 'aware', 'ceptional', 'academic', 'research', 'opportunities', 'computer', 'science', 'offered', 'xx', 'university']}, {'chunk_id': 13, 'text': 'The prospect of learning from distinguished professors like [..XX..] and [..XX..] fills me with\\nanticipation and eagerness.\\nUpon completing my post-graduation, I envision myself thoroughly equipped with the neces-\\nsary skills and resources to drive innovation and make meaningful global contributions. I aim to\\nacquire a comprehensive skill set and knowledge base that will enable me to excel in various aspects', 'start_char': 5951, 'end_char': 6362, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['prospect', 'learning', 'distinguished', 'professors', 'like', 'xx', 'xx', 'fills', 'anticipation', 'eagerness', 'upon', 'completing', 'envision', 'thoroughly', 'equipped', 'sary', 'skills', 'resources', 'drive', 'innovation', 'make', 'meaningful', 'global', 'contributions', 'aim', 'acquire', 'comprehensive', 'skill', 'set', 'knowledge', 'base', 'enable', 'excel', 'various', 'aspects']}, {'chunk_id': 14, 'text': 'of the field, from software development and data analysis to artificial intelligence and emerging\\ntechnologies. I look forward to joining [..XX..] to achieve my dreams and do impactful work.\\n2', 'start_char': 6363, 'end_char': 6555, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['field', 'software', 'development', 'data', 'analysis', 'artificial', 'intelligence', 'emerging', 'technologies', 'look', 'forward', 'joining', 'xx', 'achieve', 'dreams', 'impactful', 'work']}], 'faiss_index': <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x1681731e0> >, 'bm25_index': <rank_bm25.BM25Okapi object at 0x165097f10>}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed documents: {processed_docs}\")\n",
    "processed_docs = processor.documents\n",
    "print(f\"Processed documents: {processed_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing retriever...\n",
      "INFO:__main__:\n",
      "Testing _initialize_indices...\n",
      "INFO:__main__:Index initialization successful\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initializing retriever...\")\n",
    "retriever = ChainOfThoughtRetriever(\n",
    "    documents=processed_docs,\n",
    "    embedding_model=processor.embedding_model,\n",
    "    anthropic_client=processor.client,\n",
    "    device='cpu'\n",
    ")\n",
    "# 1. Test _initialize_indices\n",
    "logger.info(\"\\nTesting _initialize_indices...\")\n",
    "# This was called during initialization, let's verify the structures\n",
    "assert len(retriever.all_chunks) > 0, \"Chunks were not initialized\"\n",
    "assert len(retriever.doc_indices) > 0, \"Document indices were not initialized\"\n",
    "assert len(retriever.bm25_indices) > 0, \"BM25 indices were not initialized\"\n",
    "logger.info(\"Index initialization successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Testing _get_dense_results...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\n",
      "INFO:__main__:Found 3 dense results\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\nTesting _get_dense_results...\")\n",
    "dense_results = await retriever._get_dense_results(\n",
    "    query=\"what are the use cases?\",\n",
    "    k=3\n",
    ")\n",
    "logger.info(f\"Found {len(dense_results)} dense results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc...\n",
      "\n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G...\n",
      "\n",
      "Search Result #12\n",
      "Score: -1.401\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio...\n"
     ]
    }
   ],
   "source": [
    "for result in dense_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, \n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G...)\n",
      "Chunk ID: 20, Dense Rank: 2, Sparse Rank: 4\n",
      "0.031754032258064516\n",
      "(4, None)\n",
      "Chunk ID: 4, Dense Rank: 4, Sparse Rank: 1\n",
      "0.032018442622950824\n",
      "(1, \n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc...)\n",
      "Chunk ID: 22, Dense Rank: 1, Sparse Rank: 4\n",
      "0.032018442622950824\n",
      "(4, None)\n",
      "Chunk ID: 21, Dense Rank: 4, Sparse Rank: 3\n",
      "0.03149801587301587\n",
      "(3, \n",
      "Search Result #12\n",
      "Score: -1.401\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio...)\n",
      "Chunk ID: 12, Dense Rank: 3, Sparse Rank: 4\n",
      "0.03149801587301587\n",
      "(4, None)\n",
      "Chunk ID: 30, Dense Rank: 4, Sparse Rank: 2\n",
      "0.031754032258064516\n"
     ]
    }
   ],
   "source": [
    "dense_scores = {r.chunk_id: (i + 1, r) for i, r in enumerate(dense_results)}\n",
    "sparse_scores = {r.chunk_id: (i + 1, r) for i, r in enumerate(sparse_results)}\n",
    "\n",
    "# Compute reciprocal rank fuAMssion scores\n",
    "fusion_scores = {}\n",
    "for chunk_id in set(dense_scores.keys()) | set(sparse_scores.keys()):\n",
    "    print(dense_scores.get(chunk_id, (len(dense_results) + 1, None)))\n",
    "    dense_rank = dense_scores.get(chunk_id, (len(dense_results) + 1, None))[0]\n",
    "    sparse_rank = sparse_scores.get(chunk_id, (len(sparse_results) + 1, None))[0]\n",
    "    print(f\"Chunk ID: {chunk_id}, Dense Rank: {dense_rank}, Sparse Rank: {sparse_rank}\")\n",
    "    # RRF formula with k=60 (default constant)\n",
    "    fusion_scores[chunk_id] = 1 / (60 + dense_rank) + 1 / (60 + sparse_rank)\n",
    "    print(fusion_scores[chunk_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{20: 0.031754032258064516, 4: 0.032018442622950824, 22: 0.032018442622950824, 21: 0.03149801587301587, 12: 0.03149801587301587, 30: 0.031754032258064516}\n"
     ]
    }
   ],
   "source": [
    "print(fusion_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = retriever._merge_results(\n",
    "    dense_results=dense_results,\n",
    "    sparse_results=sparse_results,\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "INFO:__main__:Found 0 redundant pairs\n"
     ]
    }
   ],
   "source": [
    "# ogger.info(\"\\nTesting _check_redundancy...\")\n",
    "redundant_pairs = retriever._check_redundancy(merged_results)\n",
    "logger.info(f\"Found {len(redundant_pairs)} redundant pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Testing _get_sparse_results...\n",
      "INFO:__main__:Found 3 sparse results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result #4\n",
      "Score: 6.764\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa...\n",
      "effective retrieval.\n",
      "•Create a simple user interface facilitating interaction with the search system.\n",
      "•Evaluate and optimize the search engine’s performance using rigorous metrics to ensure high accuracy\n",
      "and efficiency.\n",
      "Use Cases\n",
      "1. Contextual Retrieval of Class Notes\n",
      "Query: ”What were the main topics covered in last week’s Machine Learning lecture?”\n",
      "•Expected Results: Class notes or lecture slides from the most recent Machine Learning lecture.\n",
      "\n",
      "Search Result #30\n",
      "Score: 3.533\n",
      "Source: 2.pdf\n",
      "Text: TCP/IP configurations. Building upon this backgrou...\n",
      "TCP/IP configurations. Building upon this background, I did some creative projects, including\n",
      "constructing my customized Linux traceroute and creating a static library that uses a reliable pro-\n",
      "tocol over UDP Sockets.\n",
      "Furthermore, I explored the vast fields of computer science. I got a glimpse into data science\n",
      "when I took artificial intelligence, machine learning natural language processing. The computer\n",
      "\n",
      "Search Result #21\n",
      "Score: 2.144\n",
      "Source: 1.pdf\n",
      "Text: Mitigation: Implement domain adaptation techniques...\n",
      "Mitigation: Implement domain adaptation techniques or fine-tune models if time permits.\n",
      "•Privacy Concerns: Users may be cautious about processing sensitive data. Mitigation: Ensure all\n",
      "operations are confined to the local environment with no external data transfers.\n",
      "Resources Needed\n",
      "Hardware/Software:\n",
      "•A development machine with at least 16 GB RAM; a dedicated GPU is advantageous.\n",
      "•Python environment with libraries: Transformers, FAISS/Milvus, PyTorch or TensorFlow.\n",
      "Data Requirements:\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\nTesting _get_sparse_results...\")\n",
    "sparse_results = retriever._get_sparse_results(\n",
    "    query=\"What is machine learning?\",\n",
    "    k=3\n",
    ")\n",
    "logger.info(f\"Found {len(sparse_results)} sparse results\")\n",
    "for result in sparse_results:\n",
    "    print(result)\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • [Definition and explanation of what AI (artificial intelligence) is]\n",
      "  • [Examples or use cases of AI]\n",
      "  • [Overview of different types/approaches to AI]\n",
      "  • [History and evolution of AI]\n",
      "  • [Current state and future potential of AI]\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What is artificial intelligence (AI)? Definition, examples, types, history and future.\"\n",
      "\n",
      "CONFIDENCE: 0.9\n",
      "\n",
      "Reasoning:\n",
      "  The provided search results are completely irrelevant to answering the query \"What is AI?\". They app...\n"
     ]
    }
   ],
   "source": [
    "reasoning_step = await retriever._get_reasoned_analysis(\n",
    "            query=\"What is AI?\",\n",
    "            results=merged_results,\n",
    "            previous_steps=[]\n",
    "        )\n",
    "print(reasoning_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20': 0.29752258064516124, '4': 0.003624828439237144, '22': 0.6, '21': 0.080889347737263, '12': -8.753511169433592, '30': 0.2}\n"
     ]
    }
   ],
   "source": [
    "combined_scores = retriever._combine_scores(\n",
    "            dense_results=dense_results,\n",
    "            sparse_results=sparse_results\n",
    "        )\n",
    "print(combined_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.39it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "results, steps = await retriever.search(\n",
    "            query=\"tell me about usecases\",\n",
    "            return_steps=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "Search Result #3\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha..., \n",
      "Search Result #0\n",
      "Score: 0.032\n",
      "Source: test_text.txt\n",
      "Text: \"\"\"The quick brown fox jumps over the lazy dog. Th..., \n",
      "Search Result #11\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: LLM(s) and Techniques: The project will utilize st..., \n",
      "Search Result #4\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa..., \n",
      "Search Result #23\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: •Final Report:\n",
      "–Introduction outlining project goa...] [\n",
      "Search Iteration\n",
      "==================================================\n",
      "\n",
      "Timestamp: 2024-11-29 16:18:25\n",
      "Query: tell me about usecases\n",
      "\n",
      "Results Summary:\n",
      "Total Results: 5\n",
      "Top Results:\n",
      "\n",
      "Search Result #3\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha...\n",
      "\n",
      "Search Result #0\n",
      "Score: 0.032\n",
      "Source: test_text.txt\n",
      "Text: \"\"\"The quick brown fox jumps over the lazy dog. Th...\n",
      "\n",
      "Search Result #11\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: LLM(s) and Techniques: The project will utilize st...\n",
      "\n",
      "Combined Scores:\n",
      "  • Chunk 0: 0.40\n",
      "  • Chunk 2: 0.00\n",
      "  • Chunk 3: 0.60\n",
      "  • Chunk 4: 0.30\n",
      "  • Chunk 8: -5.61\n",
      "  • Chunk 11: 0.20\n",
      "  • Chunk 12: -8.87\n",
      "  • Chunk 23: 0.20\n",
      "\n",
      "Reasoning Analysis:\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Additional concrete use cases beyond just retrieving class notes.\n",
      "  • - Explanation of how the search engine would handle different types of queries or documents.\n",
      "  • - Details on the user interface and how users would interact with the system.\n",
      "  • - Information on the evaluation metrics and expected performance.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"provide more examples of use cases for the local file search engine and explain how it would handle different types of queries and documents\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide some relevant information about use cases, but they are limited to a sin..., \n",
      "Search Iteration\n",
      "==================================================\n",
      "\n",
      "Timestamp: 2024-11-29 16:18:37\n",
      "Query: \"provide more examples of use cases for the local file search engine and explain how it would handle different types of queries and documents\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Results Summary:\n",
      "Total Results: 5\n",
      "Top Results:\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #8\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: 4. Search for Exam Policies\n",
      "Query: ”What are the u...\n",
      "\n",
      "Search Result #7\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: vision.”\n",
      "•Expected Results: Notes, slides, or arti...\n",
      "\n",
      "Combined Scores:\n",
      "  • Chunk 1: -3.24\n",
      "  • Chunk 2: 0.60\n",
      "  • Chunk 3: 0.10\n",
      "  • Chunk 4: 0.00\n",
      "  • Chunk 7: 0.00\n",
      "  • Chunk 8: 0.00\n",
      "  • Chunk 9: -5.24\n",
      "  • Chunk 13: 0.30\n",
      "  • Chunk 16: -4.00\n",
      "\n",
      "Reasoning Analysis:\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - More diverse examples of use cases beyond academic/student contexts.\n",
      "  • - Details on how the system would handle different document types (e.g. PDFs, images, code files).\n",
      "  • - Explanation of how the search engine ranks and presents results to the user.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"Give more examples of use cases for a local semantic file search engine in different contexts like business, software development, and personal use. Explain how it would handle diverse file types like PDFs, images, and code files. Also describe how the search results would be ranked and presented to the user.\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide a good foundation by outlining the core problem, some key use cases in a..., \n",
      "Search Iteration\n",
      "==================================================\n",
      "\n",
      "Timestamp: 2024-11-29 16:18:51\n",
      "Query: \"Give more examples of use cases for a local semantic file search engine in different contexts like business, software development, and personal use. Explain how it would handle diverse file types like PDFs, images, and code files. Also describe how the search results would be ranked and presented to the user.\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Results Summary:\n",
      "Total Results: 5\n",
      "Top Results:\n",
      "\n",
      "Search Result #4\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #7\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: vision.”\n",
      "•Expected Results: Notes, slides, or arti...\n",
      "\n",
      "Combined Scores:\n",
      "  • Chunk 1: 0.09\n",
      "  • Chunk 2: 0.20\n",
      "  • Chunk 4: 0.00\n",
      "  • Chunk 7: 0.00\n",
      "  • Chunk 9: 0.60\n",
      "  • Chunk 13: -3.79\n",
      "  • Chunk 14: -3.05\n",
      "  • Chunk 16: 0.30\n",
      "  • Chunk 20: 0.13\n",
      "  • Chunk 22: 0.07\n",
      "\n",
      "Reasoning Analysis:\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Examples of use cases in business, software development, and personal contexts.\n",
      "  • - Details on how the system would handle diverse file formats like PDFs, images, and code files.\n",
      "  • - Explanation of how search results would be ranked and presented to the user.\n",
      "\n",
      "Redundant Content:\n",
      "  • Results [Result 1, Result 2] and Both provide academic/student use case examples, prefer Result 2 as it is more detailed. overlap\n",
      "\n",
      "Suggested Refinement: \"Give examples of use cases for a local semantic file search engine in business contexts like knowledge management and research. Also, provide examples for software development like code search and documentation lookup. For personal use, describe scenarios like searching personal documents, photos, and emails. Explain how the system would handle diverse file formats like PDFs, images, code files, and how the search results would be ranked and presented.\"\n",
      "\n",
      "CONFIDENCE: 0.7\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide some relevant information on use cases and handling different file types...]\n"
     ]
    }
   ],
   "source": [
    "print(results, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Testing _merge_results...\n",
      "INFO:__main__:Merged into 3 results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result #4\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa...\n",
      "effective retrieval.\n",
      "•Create a simple user interface facilitating interaction with the search system.\n",
      "•Evaluate and optimize the search engine’s performance using rigorous metrics to ensure high accuracy\n",
      "and efficiency.\n",
      "Use Cases\n",
      "1. Contextual Retrieval of Class Notes\n",
      "Query: ”What were the main topics covered in last week’s Machine Learning lecture?”\n",
      "•Expected Results: Class notes or lecture slides from the most recent Machine Learning lecture.\n",
      "\n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc...\n",
      "Data Requirements:\n",
      "•Access to a diverse set of local files for testing (e.g., documents, reports).\n",
      "•Pre-trained LLMs accessible locally without relying on external APIs.\n",
      "Expected Deliverables\n",
      "•Code:\n",
      "–Modular and well-documented scripts for data preprocessing, embedding generation, indexing,\n",
      "query processing, and search.\n",
      "–A user interface application for interacting with the search engine.\n",
      "–Deployment instructions and a user guide.\n",
      "•Final Report:\n",
      "\n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G...\n",
      "Challenges and Risks\n",
      "•Computational Limitations: Generating embeddings for numerous files may strain resources. Mit-\n",
      "igation: Optimize code efficiency, use mini-batches, and limit initial scope if necessary.\n",
      "•Time Constraints: Developing a fully featured system in two months is challenging. Mitigation:\n",
      "Prioritize essential features; employ agile methodologies for rapid development cycles.\n",
      "•Model Adaptation: Pre-trained LLMs may not perfectly align with domain-specific language in files.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\nTesting _merge_results...\")\n",
    "merged_results = retriever._merge_results(\n",
    "    dense_results=dense_results,\n",
    "    sparse_results=sparse_results,\n",
    "    k=3\n",
    ")\n",
    "logger.info(f\"Merged into {len(merged_results)} results\")\n",
    "for result in merged_results:\n",
    "    print(result)\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "class OllamaTester:\n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    async def test_model(self, model_name: str, prompt: str, max_tokens: int = 50, temperature: float = 0.7):\n",
    "        \"\"\"Test the speed of a specific model.\"\"\"\n",
    "        url = f\"{self.base_url}/api/completions\"\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                async with session.post(url, json=payload, timeout=30) as response:\n",
    "                    if response.status != 200:\n",
    "                        print(f\"Error for model {model_name}: {response.status} - {await response.text()}\")\n",
    "                        return None\n",
    "                    result = await response.json()\n",
    "                    end_time = time.time()\n",
    "                    elapsed_time = end_time - start_time\n",
    "                    print(result)\n",
    "                    return elapsed_time, result.get('response', '')\n",
    "            except Exception as e:\n",
    "                print(f\"Error testing model {model_name}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    async def test_models(self, models: List[str], prompt: str, max_tokens: int = 50, temperature: float = 0.7):\n",
    "        \"\"\"Test multiple models and print their speeds.\"\"\"\n",
    "        results = {}\n",
    "        for model_name in models:\n",
    "            print(f\"Testing model: {model_name}...\")\n",
    "            elapsed_time, response = await self.test_model(model_name, prompt, max_tokens, temperature)\n",
    "            if elapsed_time is not None:\n",
    "                results[model_name] = {\n",
    "                    \"time\": elapsed_time,\n",
    "                    \"response_preview\": response[:100]  # Preview first 100 characters\n",
    "                }\n",
    "                print(f\"{model_name} completed in {elapsed_time:.2f} seconds.\")\n",
    "            else:\n",
    "                results[model_name] = {\"time\": None, \"response_preview\": \"Error\"}\n",
    "        return results\n",
    "\n",
    "# Define the models and prompt to test\n",
    "models_to_test = [\"qwen:0.5b\", \"llama3.2:1b\", \"llama3.2:latest\", \"mistral:7b\", \"phi3:mini\"]\n",
    "test_prompt = \"Explain the concept of chain of thought reasoning in AI.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"qwen:0.5b\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: qwen:0.5b...\n",
      "Error for model qwen:0.5b: 404 - 404 page not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tester \u001b[38;5;241m=\u001b[39m OllamaTester()\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tester\u001b[38;5;241m.\u001b[39mtest_models(models_to_test, test_prompt)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[76], line 41\u001b[0m, in \u001b[0;36mOllamaTester.test_models\u001b[0;34m(self, models, prompt, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     elapsed_time, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_model(model_name, prompt, max_tokens, temperature)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elapsed_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         results[model_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed_time,\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_preview\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[:\u001b[38;5;241m100\u001b[39m]  \u001b[38;5;66;03m# Preview first 100 characters\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         }\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "tester = OllamaTester()\n",
    "results = await tester.test_models(models_to_test, test_prompt)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for model, result in results.items():\n",
    "    print(f\"{model} - Time: {result['time']:.2f} seconds, Response Preview: {result['response_preview']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = LocalLLMClient()\n",
    "prompt = \"What is the use of AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = await llm_client.generate_response(\n",
    "            prompt=prompt,\n",
    "            max_tokens=100,\n",
    "            temperature=0.3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) has a wide range of applications and uses, which can be broadly categorized into several areas. Here are some examples:\\n\\n1. **Virtual Assistants**: AI-powered virtual assistants like Siri, Google Assistant, and Alexa enable users to perform tasks, set reminders, send messages, and more with just their voice commands.\\n2. **Image and Video Recognition**: AI algorithms can recognize objects, scenes, and emotions in images and videos, which has numerous applications in areas like surveillance, self-driving cars, and entertainment.\\n3. **Natural Language Processing (NLP)**: AI-powered NLP enables computers to understand, interpret, and generate human language, making it a crucial component in chatbots, sentiment analysis, and text translation.\\n4. **Predictive Analytics**: AI can analyze large datasets to predict future trends, behaviors, and outcomes, which is widely used in business, finance, and healthcare.\\n5. **Robotics and Automation**: AI-powered robots can perform tasks autonomously, such as assembly, logistics, and maintenance, making them ideal for industrial applications.\\n6. **Healthcare**: AI-assisted diagnosis, personalized medicine, and predictive analytics help improve patient outcomes and treatment plans in the medical field.\\n7. **Autonomous Vehicles**: AI-driven self-driving cars can navigate roads, avoid obstacles, and make decisions in real-time, revolutionizing transportation.\\n8. **Cybersecurity**: AI-powered systems can detect and respond to cyber threats in real-time, protecting networks and devices from attacks.\\n9. **Customer Service**: Chatbots and virtual assistants use AI to provide 24/7 customer support, helping businesses respond quickly to customer inquiries and issues.\\n10. **Education**: AI-powered tools can personalize learning experiences, offering adaptive assessments, recommendations, and interactive content to enhance student engagement and understanding.\\n11. **Financial Analysis**: AI can analyze financial data, identify trends, and predict market fluctuations, helping investors make more informed decisions.\\n12. **Environmental Monitoring**: AI-powered systems can track climate patterns, monitor wildlife populations, and detect natural disasters, supporting conservation efforts and emergency response planning.\\n13. **Manufacturing**: AI-driven robots and machines can optimize production processes, reduce waste, and improve product quality in industries like manufacturing and logistics.\\n14. **Space Exploration**: AI is used to analyze data from space missions, predict celestial events, and enable more efficient resource allocation for future space missions.\\n15. **Accessibility**: AI-powered tools can help people with disabilities, such as speech-to-text systems, image recognition software, and personalized learning platforms.\\n\\nThese are just a few examples of the many uses of AI. As the technology continues to evolve, we can expect to see even more innovative applications in various industries and areas.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import asyncio\n",
    "from ollama import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class OllamaBenchmark:\n",
    "    def __init__(self):\n",
    "        # Initialize the Ollama client - we'll use a single client instance\n",
    "        self.client = Client(host='http://localhost:11434')\n",
    "        \n",
    "        # Models to test - these need to be available in Ollama\n",
    "        self.models = [\n",
    "            'qwen:0.5b',\n",
    "            'llama3.2:1b',\n",
    "            'phi3:mini'\n",
    "        ]\n",
    "        \n",
    "        # Test prompts of varying complexity for benchmarking\n",
    "        self.test_prompts = [\n",
    "            \"What is 2+2?\",  # Simple arithmetic\n",
    "            \"Explain how photosynthesis works in three sentences.\",  # Medium complexity\n",
    "            \"Write a short story about a robot learning to paint.\",  # Creative/complex\n",
    "        ]\n",
    "    \n",
    "    def load_model(self, model_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Measure time taken to load a model. Using synchronous client methods\n",
    "        since the async methods are currently having compatibility issues.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Pull the model if not already present\n",
    "            # Note: Using the synchronous pull method instead of async\n",
    "            self.client.pull(model=model_name)\n",
    "            end_time = time.time()\n",
    "            return end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def run_inference(self, model_name: str, prompt: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Run a single inference and measure metrics using synchronous methods.\n",
    "        Returns timing and response length information.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Generate response using synchronous generate method\n",
    "            response = self.client.generate(\n",
    "                model=model_name,\n",
    "                prompt=prompt,\n",
    "                stream=False  # Important: Keep this false for accurate timing\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Extract the actual response text from the response object\n",
    "            response_text = response['response'] if isinstance(response, dict) else str(response)\n",
    "            \n",
    "            return {\n",
    "                'time': end_time - start_time,\n",
    "                'response_length': len(response_text)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference with {model_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def benchmark_model(self, model_name: str, num_runs: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Run complete benchmark suite for a single model, including load time\n",
    "        and multiple inference runs with different prompts.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'load_time': None,\n",
    "            'inference_times': [],\n",
    "            'tokens_per_second': [],\n",
    "            'avg_response_length': [],\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        # First, measure load time\n",
    "        print(f\"\\nLoading model {model_name}...\")\n",
    "        load_time = self.load_model(model_name)\n",
    "        results['load_time'] = load_time\n",
    "        \n",
    "        if load_time is None:\n",
    "            results['errors'].append(\"Failed to load model\")\n",
    "            return results\n",
    "        \n",
    "        # Run inference benchmarks for each prompt\n",
    "        for i, prompt in enumerate(self.test_prompts):\n",
    "            print(f\"Running prompt {i+1}/{len(self.test_prompts)}...\")\n",
    "            prompt_results = []\n",
    "            response_lengths = []\n",
    "            \n",
    "            for run in range(num_runs):\n",
    "                # Add a small delay between runs to prevent overloading\n",
    "                if run > 0:\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                inference_result = self.run_inference(model_name, prompt)\n",
    "                if inference_result:\n",
    "                    prompt_results.append(inference_result['time'])\n",
    "                    response_lengths.append(inference_result['response_length'])\n",
    "                else:\n",
    "                    results['errors'].append(f\"Failed inference on prompt {i+1}, run {run+1}\")\n",
    "            \n",
    "            if prompt_results:\n",
    "                avg_time = statistics.mean(prompt_results)\n",
    "                results['inference_times'].append(avg_time)\n",
    "                avg_length = statistics.mean(response_lengths)\n",
    "                results['avg_response_length'].append(avg_length)\n",
    "                # Estimate tokens per second (assuming ~4 chars per token)\n",
    "                tokens_per_second = (avg_length / 4) / avg_time\n",
    "                results['tokens_per_second'].append(tokens_per_second)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def run_benchmarks(self, num_runs: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Run benchmarks for all models and save detailed results to files.\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            print(f\"\\nBenchmarking {model}...\")\n",
    "            results = self.benchmark_model(model, num_runs)\n",
    "            all_results.append(results)\n",
    "            \n",
    "        # Create summary DataFrame\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        df['avg_inference_time'] = df.apply(\n",
    "            lambda x: statistics.mean(x['inference_times']) if x['inference_times'] else None, \n",
    "            axis=1\n",
    "        )\n",
    "        df['avg_tokens_per_second'] = df.apply(\n",
    "            lambda x: statistics.mean(x['tokens_per_second']) if x['tokens_per_second'] else None, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save detailed results as JSON\n",
    "        with open(f'benchmark_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "        \n",
    "        # Save summary as CSV\n",
    "        summary_df = df[['model_name', 'load_time', 'avg_inference_time', 'avg_tokens_per_second']]\n",
    "        summary_df.to_csv(f'benchmark_summary_{timestamp}.csv', index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nBenchmark Results Summary:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "        # Print any errors that occurred\n",
    "        for result in all_results:\n",
    "            if result['errors']:\n",
    "                print(f\"\\nErrors for {result['model_name']}:\")\n",
    "                for error in result['errors']:\n",
    "                    print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking qwen:0.5b...\n",
      "\n",
      "Loading model qwen:0.5b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/pull \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking llama3.2:1b...\n",
      "\n",
      "Loading model llama3.2:1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/pull \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking phi3:mini...\n",
      "\n",
      "Loading model phi3:mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/pull \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results Summary:\n",
      " model_name  load_time  avg_inference_time  avg_tokens_per_second\n",
      "  qwen:0.5b   0.645327            1.251593             210.353411\n",
      "llama3.2:1b   0.588126            7.590198             112.837565\n",
      "  phi3:mini   0.711140           24.214501              58.294165\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object NoneType can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m OllamaBenchmark()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m benchmark\u001b[38;5;241m.\u001b[39mrun_benchmarks(num_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object NoneType can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "benchmark = OllamaBenchmark()\n",
    "await benchmark.run_benchmarks(num_runs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M1) - 5455 MiB free\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./model/Meta-Llama-3-8B.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:  129 tensors\n",
      "llama_model_loader: - type q3_K:   64 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 2.95 GiB (3.16 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3024.38 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x142b448c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x142b46630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x142b45d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x142e105c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x142e14550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x142b44fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x142b47790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x142e1c5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x142e22540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x142e23310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x142b48120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x142882140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x142b492d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x142b49f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x142b4ab90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1377fe5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x142e258d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x142b4b710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x142e26740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x142b4c500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x142e27050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x142b4d090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x142b4cc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x142b4dc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x142e272b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1373a2e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1374beba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1377fe810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x142e286f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1374c24f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x142e28210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x142cb1cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137544c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137545540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142e29010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142b4eb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142e29ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142b4f490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142ccc9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1373a8410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142b4fb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137544410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142e29a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142b50140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142e2b1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142e2b950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1373ab400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1373b1220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142b510f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142e2bed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137545c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x142e2cce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x1373b67e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x142b51800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x142e2d990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142e2e240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1373bd4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1375468a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142e2ee90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142b52090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1373c5220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142cba240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142e2faa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142e2ff00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142cbc440 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1373acc40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142cac760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137547500 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142cac9c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137547910 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142cacc20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142e31020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137547f90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142cad030 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142e31610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142e32230 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142e329f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142b523e0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142e33200 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137549020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142e33bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142e33e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142cad290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142b52d50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142cad4f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142b53760 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142b53c10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142b54240 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142e34ce0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142e34fb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142e34630 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142cad750 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142b54db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142e357a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142e36ac0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142b55680 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142e374b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142b575b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142e391d0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137549dc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142b57ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142e39f40 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142e39680 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142e3a900 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13754a020 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142cad9b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142b58260 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142cadc10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142b58b30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142cade70 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13754a830 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142cae8a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142caeb00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142e3b060 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142caed60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142b59110 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142caefc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142caf220 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142caf480 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142b59e20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142caf6e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142b5a4a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142caf940 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142e3ba20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142b5aae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142e3b310 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13754b860 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142b5bce0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142b5b3f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142e3ce40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13754bac0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13754bfe0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142e3d600 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142cafba0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142cc8cc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142e3d1b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142b5c720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142cd17f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13754c810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13754d840 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13754d1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13754dd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142cd1a50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142cd1cb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x142b5e240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x142e3e140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x142b5d810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x13754eea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x142b5f0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x142b5e920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13754f2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13754f8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x142b60340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x142b60c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142e3f440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x142e3fd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142cd1f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142e40670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142b629a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142b624c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142b62f70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142b63540 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142cd2170 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142e410b0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142b647b0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142b65430 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142e41370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142e41e30 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142e424f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142b668f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1375507b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137551210 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1375518d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142b67310 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142b684d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142b67b60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142e42c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142e43940 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13754f610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142e433d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142b692e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142b69cf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142e446b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142e45b10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142b6adc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137551f50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137552c50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142e46420 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1375532a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142e46a30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142e46f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142e47b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137553c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135ebc560 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142b6c720 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137554e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1375550b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142cd23d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142b6cd60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142b6d130 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142e48200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142b6dba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142cd2630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142e48ce0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142e48950 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142e4a630 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142e492c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142e4b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142cd2890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142cd2af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142cd2d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142e4c430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142e4b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137555340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142e4bd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137555af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142b6e3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x137556650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x137557b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x142e4e670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x142cd3440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x142e4f1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x142cd3d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142e4ebe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142e4d4e0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '10', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': '.', 'llama.vocab_size': '128256'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "ggml_metal_free: deallocating\n",
      "llama_perf_context_print:        load time =    6692.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19842.19 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-1da11021-1701-419b-a90a-a61e1a631558', 'object': 'text_completion', 'created': 1732940472, 'model': './model/Meta-Llama-3-8B.Q2_K.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 12. ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 4, 'total_tokens': 17}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"./model/Meta-Llama-3-8B.Q2_K.gguf\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 10 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    6692.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  145672.95 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-8400da84-ed10-4e5b-ba16-8251c9227522', 'object': 'text_completion', 'created': 1732940720, 'model': './model/Meta-Llama-3-8B.Q2_K.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? (list in a single line) 1) Mercury 2) Venus 3) Earth 4) Mars 5) Jupiter 6) Saturn 7) Uranus 8) Neptune', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 32, 'total_tokens': 51}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? (list in a single line) \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m test_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Call the function to test the server\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtest_llm_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m, in \u001b[0;36mtest_llm_server\u001b[0;34m(url, prompt, max_tokens, temperature)\u001b[0m\n\u001b[1;32m      7\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Send the POST request\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Check for errors\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Print the response from the server\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.20/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def test_llm_server(url, prompt, max_tokens=50, temperature=0.7):\n",
    "    \"\"\"Sends a prompt to the LLM server and prints the response.\"\"\"\n",
    "    try:\n",
    "        # Define the payload for the POST request\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "\n",
    "        # Send the POST request\n",
    "        response = requests.post(url, json=payload)\n",
    "\n",
    "        # Check for errors\n",
    "        if response.status_code == 200:\n",
    "            # Print the response from the server\n",
    "            data = response.json()\n",
    "            print(\"Response from LLM:\")\n",
    "            print(data.get(\"response\", \"No response found in the output.\"))\n",
    "        else:\n",
    "            print(f\"Error: Server returned status code {response.status_code}\")\n",
    "            print(\"Details:\", response.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "\n",
    "# Server URL and prompt\n",
    "server_url = \"http://localhost:8000//v1/completions\"  # Replace with your endpoint if different\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Call the function to test the server\n",
    "test_llm_server(server_url, test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M1) - 5454 MiB free\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from model/Meta-Llama-3-8B.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:  129 tensors\n",
      "llama_model_loader: - type q3_K:   64 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 2.95 GiB (3.16 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3024.38 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x137392a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x1374ee070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1371d6a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x142e50c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1373ba910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x142cbb3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x142cba5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x142e51260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x142e514c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x142e51720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x142cbbd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x142cc2170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x142cc5fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1373b4310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x142b38ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1374f7bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x142b3a280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x1373b1e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x1373a8f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x142e51e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x142b3bbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x142e52370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x142e52850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x142e52d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x142e52f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142e531f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x142e53450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142b3c140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x142b56c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142b56230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x142e536b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x142e53910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127384910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1371ab560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1371160d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1374caf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142cc7830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142e51980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142e53b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142e53dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142e54030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142e54290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142cb6990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142e544f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142b6eee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142b6fc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1371cf940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1371cfba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1373c0580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142b70340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1374ca6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x1373ae830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x1374cbae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x142ccb450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x1373a6f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1374f6000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1374ea110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142e54750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142b71350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142e54fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142b70ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142e55220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142e55c70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142e565d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142e56eb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142e57110 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142e57370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142b72470 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142b726d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142e57890 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1374eaf90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142b73800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142b74360 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142e58240 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142b74ce0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142e59450 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142b74f40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142e59d20 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142e59f80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142e5a870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142e5b230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142e5bbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142b756f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1371d0150 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1371d09b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1371d74f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142e5d760 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1374eb1f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142e5d9c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1371d7be0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142e5e260 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1374eb950 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142e5ed30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1374ebbb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1374ebe10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142e5f590 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142e5f880 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142e601c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1374ec070 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142e60420 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1374e8c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1371d82e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1371d8990 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1374e8eb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1371d8fa0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142e61480 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142e621d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1374e9110 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142e62f20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142e63590 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1374e9370 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142e624a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142e63a70 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1374e95d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142e64470 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142b77520 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142b77c60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142b785b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142b78e90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142b79900 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1374ed310 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142e65090 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1374ed570 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142e659b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1371d9660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1371d9fc0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1371da970 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142b7b2f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142b7a880 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1374ed7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1374ee8d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1374eeb30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1371db720 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1374eed90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1374eeff0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1374ef250 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142b7be20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1374ef660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142e66e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142e672e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1374ef8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142b7d4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142b7c0d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1374f1400 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x142b7df00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x142b7f930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x1371dc4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x1371dceb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x142e67bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x1371dda70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142e690c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142e69320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x1374f1810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1374f1c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142b80b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x142e6ad80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142e6b6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1374f2a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1371df020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1374f2ce0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142b80730 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1371df6d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1374f3600 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1374f3860 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142b82310 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1374f0760 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1374f2350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1374f9080 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1374f7120 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1371df930 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142e6bf40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142b82950 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142b81b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142e6c7f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142b83950 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142e6d440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1371dfe20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1371e0080 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142b84c20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1371e1440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142b84e80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1371e0950 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142b86230 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1374f4b00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142e6e440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142b86c30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142e6f500 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142e6ff30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1374cb620 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142b870f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142b879c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142b87490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1374cdd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142e70770 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1374f74c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142e71180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1374ce9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142b88580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1371e2140 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142e71bb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142e718c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1374cec40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1374f7720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1374f5370 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142b894e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1371e3350 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142b89eb0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1371e39b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1374cfa10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1374eca50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1374fb980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1371e43f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1371e3ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142b88c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1374f84f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1371e5520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1371e5e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1371e67e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x1371e73b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x142b8c100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x142e73d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x142e75420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1371e7c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142cd6740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142e759a0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '10', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': '.', 'llama.vocab_size': '128256'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m Llama(\n\u001b[1;32m      3\u001b[0m       model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/Meta-Llama-3-8B.Q2_K.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m       chat_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an assistant who adds context to chunk based on surronunding context.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious text : There is a significant gap in the availability of local search engines that can interpret natural\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mlanguage queries and retrieve files based on semantic relevance rather than mere keyword occurrence. Ex-\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43misting solutions do not effectively leverage advancements in Large Language Models (LLMs) for local data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcurrent text : environments. Our project aims to address this challenge by developing a novel local search engine that\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mharnesses the power of LLMs to enable efficient, context-aware retrieval of files, enhancing productivity and\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdata accessibility.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mProject Objectives\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\u2022\u001b[39;49;00m\u001b[38;5;124;43mDesign and develop an innovative local file search engine that interprets natural language queries using\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mLLMs.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\u2022\u001b[39;49;00m\u001b[38;5;124;43mImplement a semantic indexing mechanism that transforms local files into meaningful embeddings for\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43meffective retrieval. \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m next text: n\u001b[39;49m\u001b[38;5;130;43;01m\\u2022\u001b[39;49;00m\u001b[38;5;124;43mCreate a simple user interface facilitating \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/llama.py:1998\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \n\u001b[1;32m   1962\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1997\u001b[0m )\n\u001b[0;32m-> 1998\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/llama_chat_format.py:637\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    633\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    634\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    635\u001b[0m         )\n\u001b[0;32m--> 637\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/llama.py:1832\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1832\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/llama.py:1317\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1318\u001b[0m     prompt_tokens,\n\u001b[1;32m   1319\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1320\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1321\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1322\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1323\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1324\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1325\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1326\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1327\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1328\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1329\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1330\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1331\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1332\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1333\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1334\u001b[0m ):\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n\u001b[1;32m   1336\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/llama.py:909\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    911\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    912\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    913\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    928\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "      model_path=\"model/Meta-Llama-3-8B.Q2_K.gguf\",\n",
    "      chat_format=\"llama-2\"\n",
    ")\n",
    "result = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who adds context to chunk based on surronunding context.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"previous text : There is a significant gap in the availability of local search engines that can interpret natural\\nlanguage queries and retrieve files based on semantic relevance rather than mere keyword occurrence. Ex-\\nisting solutions do not effectively leverage advancements in Large Language Models (LLMs) for local data\\n\\ncurrent text : environments. Our project aims to address this challenge by developing a novel local search engine that\\nharnesses the power of LLMs to enable efficient, context-aware retrieval of files, enhancing productivity and\\ndata accessibility.\\nProject Objectives\\n\\u2022Design and develop an innovative local file search engine that interprets natural language queries using\\nLLMs.\\n\\u2022Implement a semantic indexing mechanism that transforms local files into meaningful embeddings for\\neffective retrieval. \\n next text: \"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
