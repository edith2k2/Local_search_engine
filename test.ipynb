{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/battalavamshi/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from anthropic import AsyncAnthropic\n",
    "import torch\n",
    "from retriever import ChainOfThoughtRetriever\n",
    "from preprocessing import AsyncDocumentProcessor\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_search_system(processed_documents, api_key):\n",
    "    # Set up the embedding model\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    # Determine the best available device\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        embedding_model.to('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        embedding_model.to('mps')\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        embedding_model.to('cpu')\n",
    "    \n",
    "    # Initialize the Anthropic client\n",
    "    anthropic_client = AsyncAnthropic(api_key=api_key)\n",
    "    \n",
    "    # Create the retriever\n",
    "    retriever = ChainOfThoughtRetriever(\n",
    "        documents=processed_documents,\n",
    "        embedding_model=embedding_model,\n",
    "        anthropic_client=anthropic_client,\n",
    "        device=device,  # Pass the device explicitly\n",
    "        max_iterations=1,\n",
    "        results_per_step=5\n",
    "    )\n",
    "        # In your main code, after initializing the retriever\n",
    "    # print(f\"FAISS index dimension: {retriever.combined_faiss_index.d}\")\n",
    "    print(f\"Embedding model dimension: {retriever.embedding_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:Using Apple Silicon with Metal Performance Shaders\n",
      "INFO:preprocessing:Selected device for computation: mps\n",
      "INFO:preprocessing:Using device: mps\n",
      "INFO:preprocessing:Initializing with 7 processes\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/temp_uploads/Lecture 1.pdf\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing model name: sentence-transformers/all-mpnet-base-v2\n",
      "Document: /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt\n",
      "Chunk embedding shape: ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:retriever:Performing initial retrieval with query: what are the use cases\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model dimension: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "INFO:retriever:Using fusion k value: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:retriever:Performing retrieval 2 with query: \"what are some common use cases for a local file search engine that uses large language models for semantic understanding and retrieval?\"\n",
      "\n",
      "CONFIDENCE: 0.7\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "INFO:retriever:Using fusion k value: 100\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = AsyncDocumentProcessor()\n",
    "print(f\"Preprocessing model name: {processor.embedding_model_name}\")\n",
    "output_dir = Path(\"processed_documents\")\n",
    "\n",
    "# Load indices from disk\n",
    "await processor.load_indices(str(output_dir))\n",
    "\n",
    "processed_documents = processor.documents # Your preprocessed documents\n",
    "# After loading your documents\n",
    "for doc_path, doc_data in processed_documents.items():\n",
    "    for chunk in doc_data['chunks']:\n",
    "        embedding = chunk['embedding']\n",
    "        print(f\"Document: {doc_path}\")\n",
    "        print(f\"Chunk embedding shape: {np.array(embedding).shape}\")\n",
    "        break  # Just check the first chunk\n",
    "    break  # Just check the first document\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = await initialize_search_system(\n",
    "    processed_documents=processed_documents,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Perform a search\n",
    "results, reasoning_steps = await retriever.search(\n",
    "    \"what are the use cases\",\n",
    "    return_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [\n",
      "Search Result #9\n",
      "Score: 0.010\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan..., \n",
      "Search Result #12\n",
      "Score: 0.010\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa..., \n",
      "Search Result #3\n",
      "Score: 0.010\n",
      "Source: Lecture 1.pdf\n",
      "Text: oSchedule meetings for you \n",
      "oSearch the Internet \n",
      "..., \n",
      "Search Result #20\n",
      "Score: 0.010\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio..., \n",
      "Search Result #22\n",
      "Score: 0.010\n",
      "Source: 1.pdf\n",
      "Text: 6. User Interface and Results Presentation: Develo...]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "query: what are the use cases\n",
      "results\n",
      "\n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G...\n",
      "\n",
      "Search Result #12\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio...\n",
      "\n",
      "Search Result #4\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: effective retrieval.\n",
      "•Create a simple user interfa...\n",
      "\n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc...\n",
      "\n",
      "Search Result #3\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Only one specific use case is provided (retrieving class notes). Additional use cases in different domains or scenarios would be helpful.\n",
      "  • - Details on how the search engine would handle different file types or formats are missing.\n",
      "  • - Information on the user interface and how users would interact with the search engine is lacking.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"what are some potential use cases for a local file search engine that uses natural language processing and semantic embeddings?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results only provide a single use case for the proposed search engine, which is insuffic...\n",
      "combined_socre: {'2': 0.0, '3': 0.19389090909090906, '4': 0.006691616535523905, '11': -6.855537799072265, '12': 0.49767500000000003, '20': 0.48078688186459034, '22': 0.5914301043219077}\n",
      "\n",
      "Step 1\n",
      "query: \"what are some potential use cases for a local file search engine that uses natural language processing and semantic embeddings?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #30\n",
      "Score: 0.032\n",
      "Source: 2.pdf\n",
      "Text: TCP/IP configurations. Building upon this backgrou...\n",
      "\n",
      "Search Result #3\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: environments. Our project aims to address this cha...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive examples of use cases across different domains (e.g., personal, enterprise, research)\n",
      "  • - Details on how the search engine would handle different file types or formats\n",
      "  • - Potential challenges or limitations of using language models for local file search\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide a good overview of the problem and objectives but lack comprehensive cov...\n",
      "combined_socre: {'0': -6.481226492153377, '1': 0.19670041244844388, '2': 0.6, '3': 0.001023951091920664, '4': 0.13333333333333333, '9': 0.29748539497078985, '10': 0.06459807862345093, '13': 0.09170730144332073, '16': -2.7600487796809725, '30': 0.002065212832424003}\n",
      "\n",
      "Step 2\n",
      "query: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #10\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: •Implementation: File metadata indexing (submissio...\n",
      "\n",
      "Search Result #16\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: 3. “Improving Document Retrieval with Contextualiz...\n",
      "\n",
      "Search Result #7\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: vision.”\n",
      "•Expected Results: Notes, slides, or arti...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive list of key use cases across different domains (personal, enterprise, research) is missing.\n",
      "  • - Potential challenges or limitations of using NLP and semantic embeddings for local file search are not thoroughly discussed.\n",
      "  • - Details on how the system would handle different file types or formats are lacking.\n",
      "  • - Information on the specific NLP techniques or models to be used is not provided.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach? How would the system handle different file types and formats, and what specific NLP techniques or models would be used?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current search results provide some relevant information about the problem statement, motivation...\n",
      "combined_socre: {'0': -6.6494265234375, '1': 1.0, '2': 0.5911272727272727, '3': 0.05972656621390348, '7': 0.00048561809329777776, '9': -3.240650317382812, '10': 0.000734438166607892, '16': 0.2930862170087976, '20': 0.08239009222568805}\n",
      "\n",
      "Step 3\n",
      "query: \"What are some key use cases and applications for a local file search engine that leverages natural language processing and semantic embeddings across different domains (personal, enterprise, research)? What are the potential challenges or limitations of this approach? How would the system handle different file types and formats, and what specific NLP techniques or models would be used?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #10\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: •Implementation: File metadata indexing (submissio...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #7\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: vision.”\n",
      "•Expected Results: Notes, slides, or arti...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive list of key use cases across different domains (personal, enterprise, research)\n",
      "  • - Details on potential challenges or limitations beyond specific examples\n",
      "  • - Information on how the system would handle different file types and formats\n",
      "  • - Specific NLP techniques or models that would be used\n",
      "\n",
      "Redundant Content:\n",
      "  • Results  and [Result 4, Result 5] Both provide narrow, specific examples of use cases. Prefer Result 5 as it covers a slightly broader topic. overlap\n",
      "\n",
      "Suggested Refinement: \"What are the key use cases for a local file search engine leveraging natural language processing and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach? How would it handle different file formats? What specific NLP models or techniques could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide some relevant information, such as the problem motivation, proposed appr...\n",
      "combined_socre: {'0': -6.7816293189639145, '1': 0.5, '2': 0.5960494878527666, '32': 0.061010018783517854, '7': 0.0003811975813079875, '9': 0.2955266955266955, '10': 0.0011627761525143758, '11': 0.08245475238330213, '16': -3.253371150909908}\n",
      "\n",
      "Step 4\n",
      "query: \"What are the key use cases for a local file search engine leveraging natural language processing and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach? How would it handle different file formats? What specific NLP models or techniques could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "results\n",
      "\n",
      "Search Result #1\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Efficient Local File Search Engine Using Large Lan...\n",
      "\n",
      "Search Result #2\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: and document content. This limitation often leads ...\n",
      "\n",
      "Search Result #10\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: •Implementation: File metadata indexing (submissio...\n",
      "\n",
      "Search Result #9\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: rules), synonym handling (e.g., ’retake,’ ’make-up...\n",
      "\n",
      "Search Result #15\n",
      "Score: 0.031\n",
      "Source: 1.pdf\n",
      "Text: Related Work\n",
      "Literature Review:\n",
      "1. “Semantic Deskt...\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.50\n",
      "\n",
      "Relevance Findings:\n",
      "\n",
      "Identified Gaps:\n",
      "  • - Comprehensive list of key use cases across personal, enterprise, and research domains.\n",
      "  • - Potential challenges and limitations beyond specific examples.\n",
      "  • - Details on handling different file formats.\n",
      "  • - Specific NLP models or techniques that could be employed.\n",
      "\n",
      "Redundant Content:\n",
      "\n",
      "Suggested Refinement: \"What are some key use cases for a local file search engine leveraging NLP and semantic embeddings across personal, enterprise, and research domains? What are the potential challenges and limitations of this approach beyond specific examples? How would the system handle different file formats? What are some specific NLP models or techniques that could be employed?\"\n",
      "\n",
      "CONFIDENCE: 4\n",
      "\n",
      "Reasoning:\n",
      "  The current results provide a good overview of the problem statement, motivation, and proposed appro...\n",
      "combined_socre: {'0': -6.656753191266741, '1': 0.5, '2': 0.5960494878527666, '7': 0.054128913611306353, '9': 0.2955266955266955, '10': 0.0011627761525143758, '11': 0.08245475238330213, '15': 0.0003811975813079875, '16': -3.19787360297309}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for step in reasoning_steps:\n",
    "    print(f\"Step {count}\")\n",
    "    print(f\"query: {step.query}\")\n",
    "    print(\"results\")\n",
    "    for i in range(len(step.results)):\n",
    "        print(step.results[i])\n",
    "    print(step.reasoning)\n",
    "    print(f\"combined_socre: {step.combined_scores}\\n\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/battalavamshi/Desktop/TAMU/LLMs/Project/llm_search/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result #1\n",
      "Score: 0.950\n",
      "Source: doc.txt\n",
      "Text: Example text\n",
      "Context: Some context\n",
      "\n",
      "Reasoning Analysis\n",
      "====================\n",
      "\n",
      "Confidence Score: 0.92\n",
      "\n",
      "Relevance Findings:\n",
      "  • result_1: 0.95\n",
      "  • result_2: 0.85\n",
      "\n",
      "Identified Gaps:\n",
      "  • Lack of context in result_1\n",
      "  • Ambiguous phrasing in result_2\n",
      "\n",
      "Redundant Content:\n",
      "  • Results result_1 and result_2 overlap\n",
      "\n",
      "Suggested Refinement: Merge findings for conciseness.\n",
      "\n",
      "Reasoning:\n",
      "  This step evaluates the overlap between results to refine the retrieval strategy for better accuracy...\n"
     ]
    }
   ],
   "source": [
    "from temp import SearchResult, ReasoningStep, SearchIteration\n",
    "import time\n",
    "\n",
    "result = SearchResult(1, \"Example text\", \"Some context\", 0.95, \"/path/to/doc.txt\")\n",
    "print(result)\n",
    "# Output: SearchResult(id=1, score=0.950, source='doc.txt', context: Some context..., text='Example text')\n",
    "example_step = ReasoningStep(\n",
    "    relevance_findings={\"result_1\": 0.95, \"result_2\": 0.85},\n",
    "    gaps_identified=[\"Lack of context in result_1\", \"Ambiguous phrasing in result_2\"],\n",
    "    redundant_content=[(\"result_1\", \"result_2\")],\n",
    "    suggested_refinement=\"Merge findings for conciseness.\",\n",
    "    reasoning_explanation=\"This step evaluates the overlap between results to refine the retrieval strategy for better accuracy.\",\n",
    "    confidence_score=0.92\n",
    ")\n",
    "\n",
    "print(example_step)\n",
    "# Shows structured representation with all components\n",
    "\n",
    "# iteration = SearchIteration(\"query\", [result], reasoning, {\"1\": 0.9}, time.time())\n",
    "# print(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test document\n",
    "test_doc_content = \"\"\"\n",
    "Artificial Intelligence Overview\n",
    "\n",
    "AI is a broad field of computer science focused on creating intelligent machines.\n",
    "Machine learning is a subset of AI that uses data to improve performance.\n",
    "Deep learning is a type of machine learning using neural networks.\n",
    "\"\"\"\n",
    "\n",
    "# Create test directory and document\n",
    "test_dir = Path(\"test_documents\")\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "test_file = test_dir / \"test_article.txt\"\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(test_doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import AsyncDocumentProcessor, BatchConfig\n",
    "from temp import ChainOfThoughtRetriever, SearchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing:Selected device for computation: cpu\n",
      "INFO:preprocessing:Using device: cpu\n",
      "INFO:preprocessing:Initializing with 7 processes\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:__main__:Processing test document...\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf\n",
      "INFO:preprocessing:Successfully loaded indices for /Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor for document processing\n",
    "processor = AsyncDocumentProcessor(\n",
    "    embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    anthropic_api_key=api_key,\n",
    "    device='cpu',  # Using CPU for testing\n",
    "    batch_config=BatchConfig(\n",
    "        embeddings=32,\n",
    "        context=10,\n",
    "        faiss=1000,\n",
    "        documents=5,\n",
    "        process=4\n",
    "    ),\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=2\n",
    ")\n",
    "test_dir = Path(\"processed_documents\")\n",
    "# Process test document\n",
    "logger.info(\"Processing test document...\")\n",
    "processed_docs = await processor.load_indices(str(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: None\n",
      "Processed documents: {'/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt': {'metadata': {'file_path': '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/test_text.txt', 'file_name': 'test_text.txt', 'file_type': '.txt', 'created_time': '2024-11-24T11:57:27.521049', 'modified_time': '2024-11-24T11:57:27.521049', 'size_bytes': 475, 'num_chunks': 1, 'processing_time': 5.821021, 'batch_sizes': {'embeddings': 32, 'context': 10, 'faiss': 1000, 'documents': 5, 'process': 2}}, 'chunks': [{'chunk_id': 0, 'text': '\"\"\"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the English alphabet, making it a popular pangram. It\\'s often used for typing practice, font displays, and testing equipment. While simple, this sentence serves as a great tool for showcasing how all the letters are used in different contexts.\\n    It’s a fun and quirky way to test a variety of systems and applications that require the use of all characters in the English alphabet.\"\"\"', 'start_char': 0, 'end_char': 473, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'sentence', 'contains', 'every', 'letter', 'english', 'alphabet', 'making', 'popular', 'pangram', 'often', 'used', 'typing', 'practice', 'font', 'displays', 'testing', 'equipment', 'simple', 'sentence', 'serves', 'great', 'tool', 'showcasing', 'letters', 'used', 'different', 'contexts', 'fun', 'quirky', 'way', 'test', 'variety', 'systems', 'applications', 'require', 'use', 'characters', 'english', 'alphabet']}], 'faiss_index': <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x16505ac30> >, 'bm25_index': <rank_bm25.BM25Okapi object at 0x16505aa30>}, '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf': {'metadata': {'file_path': '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/1.pdf', 'file_name': '1.pdf', 'file_type': '.pdf', 'created_time': '2024-11-24T09:53:19.406365', 'modified_time': '2024-10-06T02:48:24.092947', 'size_bytes': 89986, 'num_chunks': 23, 'processing_time': 23.03453, 'batch_sizes': {'embeddings': 32, 'context': 10, 'faiss': 1000, 'documents': 5, 'process': 2}}, 'chunks': [{'chunk_id': 0, 'text': 'Efficient Local File Search Engine Using Large Language Models\\nProblem Statement\\nContext: With the exponential growth of digital information, individuals and organizations store vast\\namounts of data locally on personal devices and enterprise servers. Traditional file search tools rely heavily\\non exact keyword matching, lacking the ability to understand the context or semantics behind user queries', 'start_char': 0, 'end_char': 399, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['efficient', 'local', 'file', 'search', 'engine', 'using', 'large', 'language', 'models', 'problem', 'statement', 'context', 'exponential', 'growth', 'digital', 'information', 'individuals', 'organizations', 'store', 'vast', 'amounts', 'data', 'locally', 'personal', 'devices', 'enterprise', 'servers', 'traditional', 'file', 'search', 'tools', 'rely', 'heavily', 'exact', 'keyword', 'matching', 'lacking', 'ability', 'understand', 'context', 'semantics', 'behind', 'user', 'queries']}, {'chunk_id': 1, 'text': 'and document content. This limitation often leads to inefficient retrieval processes, where users struggle to\\nlocate relevant files amidst overwhelming data volumes.\\nProblem: There is a significant gap in the availability of local search engines that can interpret natural\\nlanguage queries and retrieve files based on semantic relevance rather than mere keyword occurrence. Ex-\\nisting solutions do not effectively leverage advancements in Large Language Models (LLMs) for local data', 'start_char': 400, 'end_char': 882, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['document', 'content', 'limitation', 'often', 'leads', 'inefficient', 'retrieval', 'processes', 'users', 'struggle', 'locate', 'relevant', 'files', 'amidst', 'overwhelming', 'data', 'volumes', 'problem', 'significant', 'gap', 'availability', 'local', 'search', 'engines', 'interpret', 'natural', 'language', 'queries', 'retrieve', 'files', 'based', 'semantic', 'relevance', 'rather', 'mere', 'keyword', 'occurrence', 'isting', 'solutions', 'effectively', 'leverage', 'advancements', 'large', 'language', 'models', 'llms', 'local', 'data']}, {'chunk_id': 2, 'text': 'environments. Our project aims to address this challenge by developing a novel local search engine that\\nharnesses the power of LLMs to enable efficient, context-aware retrieval of files, enhancing productivity and\\ndata accessibility.\\nProject Objectives\\n•Design and develop an innovative local file search engine that interprets natural language queries using\\nLLMs.\\n•Implement a semantic indexing mechanism that transforms local files into meaningful embeddings for\\neffective retrieval.', 'start_char': 883, 'end_char': 1368, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['environments', 'project', 'aims', 'address', 'challenge', 'developing', 'novel', 'local', 'search', 'engine', 'harnesses', 'power', 'llms', 'enable', 'efficient', 'retrieval', 'files', 'enhancing', 'productivity', 'data', 'accessibility', 'project', 'objectives', 'develop', 'innovative', 'local', 'file', 'search', 'engine', 'interprets', 'natural', 'language', 'queries', 'using', 'llms', 'semantic', 'indexing', 'mechanism', 'transforms', 'local', 'files', 'meaningful', 'embeddings', 'effective', 'retrieval']}, {'chunk_id': 3, 'text': 'effective retrieval.\\n•Create a simple user interface facilitating interaction with the search system.\\n•Evaluate and optimize the search engine’s performance using rigorous metrics to ensure high accuracy\\nand efficiency.\\nUse Cases\\n1. Contextual Retrieval of Class Notes\\nQuery: ”What were the main topics covered in last week’s Machine Learning lecture?”\\n•Expected Results: Class notes or lecture slides from the most recent Machine Learning lecture.', 'start_char': -1, 'end_char': 447, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['effective', 'retrieval', 'simple', 'user', 'interface', 'facilitating', 'interaction', 'search', 'system', 'optimize', 'search', 'engine', 'performance', 'using', 'rigorous', 'metrics', 'ensure', 'high', 'accuracy', 'efficiency', 'use', 'cases', 'contextual', 'retrieval', 'class', 'notes', 'query', 'main', 'topics', 'covered', 'last', 'week', 'machine', 'learning', 'lecture', 'results', 'class', 'notes', 'lecture', 'slides', 'recent', 'machine', 'learning', 'lecture']}, {'chunk_id': 4, 'text': '•Challenges: Temporal understanding (finding the correct week’s material), identifying important\\ntopics, filtering irrelevant notes.\\n•Implementation: Date-based filtering of notes, key topic extraction from lecture content.\\n2. Semantic Search for Assignment Guidelines\\nQuery: ”Find the guidelines for the upcoming Operating Systems project.”\\n•Expected Results: Documents or web links containing project guidelines or assignment descriptions.', 'start_char': 1797, 'end_char': 2238, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['temporal', 'understanding', 'finding', 'correct', 'week', 'material', 'identifying', 'important', 'topics', 'filtering', 'irrelevant', 'notes', 'filtering', 'notes', 'key', 'topic', 'extraction', 'lecture', 'content', 'semantic', 'search', 'assignment', 'guidelines', 'query', 'find', 'guidelines', 'upcoming', 'operating', 'systems', 'results', 'documents', 'web', 'links', 'containing', 'project', 'guidelines', 'assignment', 'descriptions']}, {'chunk_id': 5, 'text': '•Challenges: Differentiating between similar projects, recognizing assignment-specific terms.\\n1 •Implementation: Natural language processing for identifying assignment names, course-specific key-\\nword mapping, version control to find the latest guidelines.\\n3. Cross-Document Study Material Discovery\\nQuery: ”Show me all notes related to convolutional neural networks and their applications in computer\\nvision.”', 'start_char': 2239, 'end_char': 2649, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['differentiating', 'similar', 'projects', 'recognizing', 'terms', 'natural', 'language', 'processing', 'identifying', 'assignment', 'names', 'word', 'mapping', 'version', 'control', 'find', 'latest', 'guidelines', 'study', 'material', 'discovery', 'query', 'show', 'notes', 'related', 'convolutional', 'neural', 'networks', 'applications', 'computer', 'vision']}, {'chunk_id': 6, 'text': 'vision.”\\n•Expected Results: Notes, slides, or articles covering convolutional neural networks (CNNs) and\\ncomputer vision examples from various classes or study materials.\\n•Challenges: Linking concepts across different documents and subjects, handling related terms like\\n‘CNN’ and ‘image recognition.’\\n•Implementation: Entity recognition for key terms (CNN, computer vision), cross-document linking,\\ntopic modeling to identify relevant content.\\n4. Search for Exam Policies', 'start_char': -1, 'end_char': 470, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['results', 'notes', 'slides', 'articles', 'covering', 'convolutional', 'neural', 'networks', 'cnns', 'computer', 'vision', 'examples', 'various', 'classes', 'study', 'materials', 'linking', 'concepts', 'across', 'different', 'documents', 'subjects', 'handling', 'related', 'terms', 'like', 'cnn', 'image', 'entity', 'recognition', 'key', 'terms', 'cnn', 'computer', 'vision', 'linking', 'topic', 'modeling', 'identify', 'relevant', 'content', 'search', 'exam', 'policies']}, {'chunk_id': 7, 'text': '4. Search for Exam Policies\\nQuery: ”What are the university’s rules on retaking exams?”\\n•Expected Results: University or department policy documents that explain the rules for retaking\\nexams or handling missed exams.\\n•Challenges: Understanding different policy terminologies, finding up-to-date documents, differenti-\\nating between departments.\\n•Implementation: Policy document segmentation, time-based document retrieval (to find the latest', 'start_char': 3085, 'end_char': 3526, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['search', 'exam', 'policies', 'query', 'university', 'rules', 'retaking', 'exams', 'results', 'university', 'department', 'policy', 'documents', 'explain', 'rules', 'retaking', 'exams', 'handling', 'missed', 'exams', 'understanding', 'different', 'policy', 'terminologies', 'finding', 'documents', 'ating', 'departments', 'policy', 'document', 'segmentation', 'document', 'retrieval', 'find', 'latest']}, {'chunk_id': 8, 'text': 'rules), synonym handling (e.g., ’retake,’ ’make-up exam’).\\n5. Personalized Search for Assignment Submissions\\nQuery: ”Retrieve all the reports I submitted for the Data Structures course last semester.”\\n•Expected Results: Submitted reports or project files for the Data Structures course from the previous\\nsemester.\\n•Challenges: Time frame filtering, identifying the correct course, user-specific data filtering.', 'start_char': 3527, 'end_char': 3937, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['rules', 'synonym', 'handling', 'retake', 'exam', 'personalized', 'search', 'assignment', 'submissions', 'query', 'retrieve', 'reports', 'submitted', 'data', 'structures', 'course', 'last', 'results', 'submitted', 'reports', 'project', 'files', 'data', 'structures', 'course', 'previous', 'semester', 'time', 'frame', 'filtering', 'identifying', 'correct', 'course', 'data', 'filtering']}, {'chunk_id': 9, 'text': '•Implementation: File metadata indexing (submission dates, course tags), project tagging, user-\\nspecific search.\\nMethodology\\nApproach: We propose to build a novel semantic search system tailored for local file environments, lever-\\naging LLMs to understand and process natural language queries. This system will bridge the gap between\\nadvanced language understanding models and local data search, a relatively unexplored domain.', 'start_char': 3938, 'end_char': 4365, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['file', 'metadata', 'indexing', 'submission', 'dates', 'course', 'tags', 'project', 'tagging', 'specific', 'search', 'methodology', 'approach', 'propose', 'build', 'novel', 'semantic', 'search', 'system', 'tailored', 'local', 'file', 'environments', 'aging', 'llms', 'understand', 'process', 'natural', 'language', 'queries', 'system', 'bridge', 'gap', 'advanced', 'language', 'understanding', 'models', 'local', 'data', 'search', 'relatively', 'unexplored', 'domain']}, {'chunk_id': 10, 'text': 'LLM(s) and Techniques: The project will utilize state-of-the-art open-source LLMs such as OpenAI’s\\nGPT-4 or the latest NVIDIA’s NV-embed for generating text embeddings. Techniques like Sentence Trans-\\nformers will be used to create embeddings that capture the semantic essence of documents. Libraries such as\\nHugging Face Transformers and FAISS/Milvus will be employed for efficient indexing and similarity search\\noperations.\\nArchitecture/Process:', 'start_char': 4366, 'end_char': 4813, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['llm', 'techniques', 'project', 'utilize', 'llms', 'openai', 'latest', 'nvidia', 'generating', 'text', 'embeddings', 'techniques', 'like', 'sentence', 'formers', 'used', 'create', 'embeddings', 'capture', 'semantic', 'essence', 'documents', 'libraries', 'hugging', 'face', 'transformers', 'employed', 'efficient', 'indexing', 'similarity', 'search', 'operations']}, {'chunk_id': 11, 'text': 'operations.\\nArchitecture/Process:\\n1. Data Ingestion and Preprocessing: Collect and parse local files in various formats (e.g., .txt, .pdf,\\n.docx). Preprocess the text to clean and normalize content, handling encoding issues and extracting\\nmeaningful text.\\n2 2. Semantic Embedding Generation: Use LLMs to convert preprocessed text into high-dimensional se-\\nmantic embeddings that reflect the contextual meaning of documents.', 'start_char': -1, 'end_char': 422, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['operations', 'data', 'ingestion', 'preprocessing', 'collect', 'parse', 'local', 'files', 'various', 'formats', 'preprocess', 'text', 'clean', 'normalize', 'content', 'handling', 'encoding', 'issues', 'extracting', 'meaningful', 'text', 'semantic', 'embedding', 'generation', 'use', 'llms', 'convert', 'preprocessed', 'text', 'mantic', 'embeddings', 'reflect', 'contextual', 'meaning', 'documents']}, {'chunk_id': 12, 'text': '3. Indexing: Store these embeddings in a vector database, enabling efficient similarity searches through\\napproximate nearest neighbor algorithms.\\n4. Natural Language Query Processing: Accept user queries in plain language, generate query embeddings\\nusing the same LLM to ensure consistency in semantic space.\\n5. Semantic Search and Ranking: Perform similarity searches to find embeddings closest to the query\\nembedding and rank the results based on relevance scores.', 'start_char': 5204, 'end_char': 5670, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['indexing', 'store', 'embeddings', 'vector', 'database', 'enabling', 'efficient', 'similarity', 'searches', 'approximate', 'nearest', 'neighbor', 'algorithms', 'natural', 'language', 'query', 'processing', 'accept', 'user', 'queries', 'plain', 'language', 'generate', 'query', 'embeddings', 'using', 'llm', 'ensure', 'consistency', 'semantic', 'space', 'semantic', 'search', 'ranking', 'perform', 'similarity', 'searches', 'find', 'embeddings', 'closest', 'query', 'embedding', 'rank', 'results', 'based', 'relevance', 'scores']}, {'chunk_id': 13, 'text': '6. User Interface and Results Presentation: Develop an interface that displays search results with con-\\ntextually relevant snippets, allowing users to preview content.\\nData: The data comprises the user’s local files, ensuring privacy and security as all processing occurs locally.\\nNo external data transmission is involved.\\nEvaluation: Success will be measured using evaluation metrics like Mean Reciprocal Rank (MRR), Preci-\\nsion@K, and user satisfaction scores.\\nRelated Work\\nLiterature Review:', 'start_char': 5671, 'end_char': 6166, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['user', 'interface', 'results', 'presentation', 'develop', 'interface', 'displays', 'search', 'results', 'textually', 'relevant', 'snippets', 'allowing', 'users', 'preview', 'content', 'data', 'data', 'comprises', 'user', 'local', 'files', 'ensuring', 'privacy', 'security', 'processing', 'occurs', 'locally', 'external', 'data', 'transmission', 'involved', 'evaluation', 'success', 'measured', 'using', 'evaluation', 'metrics', 'like', 'mean', 'reciprocal', 'rank', 'mrr', 'sion', 'k', 'user', 'satisfaction', 'scores', 'related', 'work', 'literature', 'review']}, {'chunk_id': 14, 'text': 'Related Work\\nLiterature Review:\\n1. “Semantic Desktop Search: A New Paradigm” by Smith et al. (2020) discusses applying semantic\\nsearch to desktop environments using ontology-based methods, highlighting limitations in handling\\nunstructured data.\\n2. “Leveraging Transformer Models for Information Retrieval” by Lee and Chen (2021) explores transformer-\\nbased models in web search engines but does not focus on local file systems or privacy concerns.', 'start_char': -1, 'end_char': 446, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['related', 'work', 'literature', 'review', 'semantic', 'desktop', 'search', 'new', 'paradigm', 'smith', 'et', 'al', 'discusses', 'applying', 'semantic', 'search', 'desktop', 'environments', 'using', 'methods', 'highlighting', 'limitations', 'handling', 'unstructured', 'data', 'leveraging', 'transformer', 'models', 'information', 'retrieval', 'lee', 'chen', 'explores', 'based', 'models', 'web', 'search', 'engines', 'focus', 'local', 'file', 'systems', 'privacy', 'concerns']}, {'chunk_id': 15, 'text': '3. “Improving Document Retrieval with Contextualized Language Representations” by Zhang and Liu\\n(2019) investigates contextual embeddings for document retrieval in cloud services, lacking application\\nto local environments.\\nPositioning: Our project differentiates itself by being among the first to integrate advanced LLMs into a\\nlocal file search engine, providing a novel application of these models in a domain that has not fully utilized', 'start_char': 6583, 'end_char': 7023, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['improving', 'document', 'retrieval', 'contextualized', 'language', 'representations', 'zhang', 'liu', 'investigates', 'contextual', 'embeddings', 'document', 'retrieval', 'cloud', 'services', 'lacking', 'application', 'local', 'environments', 'positioning', 'project', 'differentiates', 'among', 'first', 'integrate', 'advanced', 'llms', 'local', 'file', 'search', 'engine', 'providing', 'novel', 'application', 'models', 'domain', 'fully', 'utilized']}, {'chunk_id': 16, 'text': 'their capabilities. We aim to fill the gap identified in existing literature by bringing semantic understanding\\nto personal and organizational file search, enhancing the efficiency and accuracy of information retrieval in\\nlocal environments.\\nTimeline\\nPhase 1 (Weeks 1–4):\\n1. Week 1: Finalize project requirements, select appropriate LLMs, and design system architecture.\\n2. Week 2: Develop data ingestion and preprocessing pipelines; initiate embedding generation for a sample\\ndataset.', 'start_char': 7024, 'end_char': 7509, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['capabilities', 'aim', 'fill', 'gap', 'identified', 'existing', 'literature', 'bringing', 'semantic', 'understanding', 'personal', 'organizational', 'file', 'search', 'enhancing', 'efficiency', 'accuracy', 'information', 'retrieval', 'local', 'environments', 'timeline', 'phase', 'weeks', 'week', 'finalize', 'project', 'requirements', 'select', 'appropriate', 'llms', 'design', 'system', 'architecture', 'week', 'develop', 'data', 'ingestion', 'preprocessing', 'pipelines', 'initiate', 'embedding', 'generation', 'sample', 'dataset']}, {'chunk_id': 17, 'text': 'dataset.\\n3. Week 3: Complete embedding generation for all target files; implement the vector database indexing.\\n4. Week 4: Develop basic query processing and search functionalities; conduct initial testing.\\nPhase 2 (Weeks 5–8):\\n1. Week 5: Enhance query processing to handle complex queries; optimize embedding generation.\\n2. Week 6: Design and implement the user interface; integrate frontend and backend components.\\n3 3. Week 7: Conduct performance optimization; implement feedback mechanisms.', 'start_char': -1, 'end_char': 493, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['dataset', 'week', 'complete', 'embedding', 'generation', 'target', 'files', 'implement', 'vector', 'database', 'indexing', 'week', 'develop', 'basic', 'query', 'processing', 'search', 'functionalities', 'conduct', 'initial', 'testing', 'phase', 'weeks', 'week', 'enhance', 'query', 'processing', 'handle', 'complex', 'queries', 'optimize', 'embedding', 'generation', 'week', 'design', 'implement', 'user', 'interface', 'integrate', 'frontend', 'backend', 'components', 'week', 'conduct', 'performance', 'optimization', 'implement', 'feedback', 'mechanisms']}, {'chunk_id': 18, 'text': '4. Week 8: Perform comprehensive testing and evaluation; prepare final report and presentation materials.\\nMilestones:\\n1. Week 2: Completion of data preprocessing and initial embedding generation.\\n2. Week 4: Operational prototype of the search engine with basic functionalities.\\n3. Week 6: User interface completed and integrated with the backend.\\n4. Week 8: Final system evaluation and submission of deliverables.\\nChallenges and Risks', 'start_char': 7996, 'end_char': 8430, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['week', 'perform', 'comprehensive', 'testing', 'evaluation', 'prepare', 'final', 'report', 'presentation', 'materials', 'milestones', 'week', 'completion', 'data', 'preprocessing', 'initial', 'embedding', 'generation', 'week', 'operational', 'prototype', 'search', 'engine', 'basic', 'functionalities', 'week', 'user', 'interface', 'completed', 'integrated', 'backend', 'week', 'final', 'system', 'evaluation', 'submission', 'deliverables', 'challenges', 'risks']}, {'chunk_id': 19, 'text': 'Challenges and Risks\\n•Computational Limitations: Generating embeddings for numerous files may strain resources. Mit-\\nigation: Optimize code efficiency, use mini-batches, and limit initial scope if necessary.\\n•Time Constraints: Developing a fully featured system in two months is challenging. Mitigation:\\nPrioritize essential features; employ agile methodologies for rapid development cycles.\\n•Model Adaptation: Pre-trained LLMs may not perfectly align with domain-specific language in files.', 'start_char': -1, 'end_char': 490, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['challenges', 'risks', 'limitations', 'generating', 'embeddings', 'numerous', 'files', 'may', 'strain', 'resources', 'igation', 'optimize', 'code', 'efficiency', 'use', 'limit', 'initial', 'scope', 'necessary', 'constraints', 'developing', 'fully', 'featured', 'system', 'two', 'months', 'challenging', 'mitigation', 'prioritize', 'essential', 'features', 'employ', 'agile', 'methodologies', 'rapid', 'development', 'cycles', 'adaptation', 'llms', 'may', 'perfectly', 'align', 'language', 'files']}, {'chunk_id': 20, 'text': 'Mitigation: Implement domain adaptation techniques or fine-tune models if time permits.\\n•Privacy Concerns: Users may be cautious about processing sensitive data. Mitigation: Ensure all\\noperations are confined to the local environment with no external data transfers.\\nResources Needed\\nHardware/Software:\\n•A development machine with at least 16 GB RAM; a dedicated GPU is advantageous.\\n•Python environment with libraries: Transformers, FAISS/Milvus, PyTorch or TensorFlow.\\nData Requirements:', 'start_char': 8902, 'end_char': 9391, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['mitigation', 'implement', 'domain', 'adaptation', 'techniques', 'models', 'time', 'permits', 'concerns', 'users', 'may', 'cautious', 'processing', 'sensitive', 'data', 'mitigation', 'ensure', 'operations', 'confined', 'local', 'environment', 'external', 'data', 'transfers', 'resources', 'needed', 'development', 'machine', 'least', 'gb', 'ram', 'dedicated', 'gpu', 'advantageous', 'environment', 'libraries', 'transformers', 'pytorch', 'tensorflow', 'data', 'requirements']}, {'chunk_id': 21, 'text': 'Data Requirements:\\n•Access to a diverse set of local files for testing (e.g., documents, reports).\\n•Pre-trained LLMs accessible locally without relying on external APIs.\\nExpected Deliverables\\n•Code:\\n–Modular and well-documented scripts for data preprocessing, embedding generation, indexing,\\nquery processing, and search.\\n–A user interface application for interacting with the search engine.\\n–Deployment instructions and a user guide.\\n•Final Report:', 'start_char': -1, 'end_char': 448, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['data', 'requirements', 'diverse', 'set', 'local', 'files', 'testing', 'documents', 'reports', 'llms', 'accessible', 'locally', 'without', 'relying', 'external', 'apis', 'expected', 'deliverables', 'scripts', 'data', 'preprocessing', 'embedding', 'generation', 'indexing', 'query', 'processing', 'search', 'user', 'interface', 'application', 'interacting', 'search', 'engine', 'instructions', 'user', 'guide', 'report']}, {'chunk_id': 22, 'text': '•Final Report:\\n–Introduction outlining project goals and significance.\\n–Detailed methodology explaining technical implementations and architectural decisions.\\n–Experimental results with evaluation metrics and analysis.\\n–Discussion of challenges faced, solutions implemented, and potential future enhancements.\\n–Conclusion summarizing findings and contributions.\\n4', 'start_char': 9808, 'end_char': 10171, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['report', 'outlining', 'project', 'goals', 'significance', 'methodology', 'explaining', 'technical', 'implementations', 'architectural', 'decisions', 'results', 'evaluation', 'metrics', 'analysis', 'challenges', 'faced', 'solutions', 'implemented', 'potential', 'future', 'enhancements', 'summarizing', 'findings', 'contributions']}], 'faiss_index': <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x168463420> >, 'bm25_index': <rank_bm25.BM25Okapi object at 0x1684855e0>}, '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf': {'metadata': {'file_path': '/Users/battalavamshi/Desktop/TAMU/LLMs/Project/pdfs/2.pdf', 'file_name': '2.pdf', 'file_type': '.pdf', 'created_time': '2024-11-24T09:53:23.037822', 'modified_time': '2023-07-01T11:29:13.937205', 'size_bytes': 31250, 'num_chunks': 15, 'processing_time': 29.406026, 'batch_sizes': {'embeddings': 32, 'context': 10, 'faiss': 1000, 'documents': 5, 'process': 2}}, 'chunks': [{'chunk_id': 0, 'text': 'STATEMENT OF PURPOSE\\nBattala Vamshi Krishna\\n”Computer science empowers students to create the world of tomorrow.”\\n- Satya Nadella, CEO of Microsoft\\nI got glimpses of the brilliant world of computers at a very early stage when I stumbled upon the\\nremains of a computer lying in my house’s storage area. I vividly remember collecting it and trying\\nto make sense out of it. Despite being unable to accomplish much, my curiosity did not stop me.', 'start_char': 0, 'end_char': 441, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['statement', 'purpose', 'battala', 'vamshi', 'krishna', 'computer', 'science', 'empowers', 'students', 'create', 'world', 'satya', 'nadella', 'ceo', 'microsoft', 'got', 'glimpses', 'brilliant', 'world', 'computers', 'early', 'stage', 'stumbled', 'upon', 'remains', 'computer', 'lying', 'house', 'storage', 'area', 'vividly', 'remember', 'collecting', 'trying', 'make', 'sense', 'despite', 'unable', 'accomplish', 'much', 'curiosity', 'stop']}, {'chunk_id': 1, 'text': 'Throughout my formative years, I immersed myself in beginner-level books and online resources,\\navidly learning about computers and technology, realizing that I needed to take my knowledge to\\nthe next level.\\nMy journey with computers brought me to IIT Kharagpur, one of India’s most prestigious in-\\nstitutes. Enrolling in their Bachelor of Technology program in Computer Science marked a sig-\\nnificant turning point in my knowledge pursuit. While at IIT Kharagpur, I was immersed in a', 'start_char': 442, 'end_char': 925, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['throughout', 'formative', 'years', 'immersed', 'books', 'online', 'resources', 'avidly', 'learning', 'computers', 'technology', 'realizing', 'needed', 'take', 'knowledge', 'next', 'level', 'journey', 'computers', 'brought', 'iit', 'kharagpur', 'one', 'india', 'prestigious', 'stitutes', 'enrolling', 'bachelor', 'technology', 'program', 'computer', 'science', 'marked', 'nificant', 'turning', 'point', 'knowledge', 'pursuit', 'iit', 'kharagpur', 'immersed']}, {'chunk_id': 2, 'text': 'comprehensive curriculum covering various computer science subjects. From fundamental courses\\nin programming & data structures, and algorithms to advanced topics such as operating systems,\\ndatabases, and Natural Language Processing, I have achieved a high level of understanding of fun-\\ndamental principles in computer science. Over my initial semesters, I gradually enhanced my pro-\\ngramming abilities and sharpened my skills in analyzing algorithms. As I progressed in my studies,', 'start_char': 926, 'end_char': 1408, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['comprehensive', 'curriculum', 'covering', 'various', 'computer', 'science', 'subjects', 'fundamental', 'courses', 'programming', 'data', 'structures', 'algorithms', 'advanced', 'topics', 'operating', 'systems', 'databases', 'natural', 'language', 'processing', 'achieved', 'high', 'level', 'understanding', 'damental', 'principles', 'computer', 'science', 'initial', 'semesters', 'gradually', 'enhanced', 'gramming', 'abilities', 'sharpened', 'skills', 'analyzing', 'algorithms', 'progressed', 'studies']}, {'chunk_id': 3, 'text': 'I had the opportunity to explore advanced topics in computer science.\\nI got my first taste of lower-level programming in my fifth semester when I took the computer\\norganization & architecture course, instructed by Professor Bhargab Bikram Bhattacharya. This\\ncourse allowed me to delve deep into the principles of computer design & intricacies of proces-\\nsor functionality. Drawing on this knowledge, I developed a single-cycle processor, meticulously', 'start_char': 1409, 'end_char': 1859, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['opportunity', 'explore', 'advanced', 'topics', 'computer', 'science', 'got', 'first', 'taste', 'programming', 'fifth', 'semester', 'took', 'computer', 'organization', 'architecture', 'course', 'instructed', 'professor', 'bhargab', 'bikram', 'bhattacharya', 'course', 'allowed', 'delve', 'deep', 'principles', 'computer', 'design', 'intricacies', 'sor', 'functionality', 'drawing', 'knowledge', 'developed', 'processor', 'meticulously']}, {'chunk_id': 4, 'text': 'crafting the datapath and control signals to ensure seamless instruction execution. Synthesizing the\\nVerilog code on the Xilinx ISE validated my design and gave me valuable insights into the hardware\\nimplementation of a processor.\\nDuring the subsequent semester, I immersed myself in the systems aspect of computers through\\ncourses like Operating Systems (OS) and Computer Networks. In the OS course, I understood', 'start_char': 1860, 'end_char': 2273, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['crafting', 'datapath', 'control', 'signals', 'ensure', 'seamless', 'instruction', 'execution', 'synthesizing', 'verilog', 'code', 'xilinx', 'ise', 'validated', 'design', 'gave', 'valuable', 'insights', 'hardware', 'implementation', 'processor', 'subsequent', 'semester', 'immersed', 'systems', 'aspect', 'computers', 'courses', 'like', 'operating', 'systems', 'os', 'computer', 'networks', 'os', 'course', 'understood']}, {'chunk_id': 5, 'text': 'many concepts, encompassing everything from syscalls to memory management and filesystems.\\nThis knowledge enabled me to undertake exciting projects, including developing shells, working\\nwith shared memory and threads, and creating memory management systems. The computer net-\\nworks course taught me about the interconnection and communication aspects between multiple\\ncomputers. These included concepts ranging from basic medium-sharing protocols to advanced', 'start_char': 2274, 'end_char': 2732, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['many', 'concepts', 'encompassing', 'everything', 'syscalls', 'memory', 'management', 'filesystems', 'knowledge', 'enabled', 'undertake', 'exciting', 'projects', 'including', 'developing', 'shells', 'working', 'shared', 'memory', 'threads', 'creating', 'memory', 'management', 'systems', 'computer', 'works', 'course', 'taught', 'interconnection', 'communication', 'aspects', 'multiple', 'computers', 'included', 'concepts', 'ranging', 'basic', 'protocols', 'advanced']}, {'chunk_id': 6, 'text': 'TCP/IP configurations. Building upon this background, I did some creative projects, including\\nconstructing my customized Linux traceroute and creating a static library that uses a reliable pro-\\ntocol over UDP Sockets.\\nFurthermore, I explored the vast fields of computer science. I got a glimpse into data science\\nwhen I took artificial intelligence, machine learning natural language processing. The computer', 'start_char': 2733, 'end_char': 3141, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['configurations', 'building', 'upon', 'background', 'creative', 'projects', 'including', 'constructing', 'customized', 'linux', 'traceroute', 'creating', 'static', 'library', 'uses', 'reliable', 'tocol', 'udp', 'sockets', 'furthermore', 'explored', 'vast', 'fields', 'computer', 'science', 'got', 'glimpse', 'data', 'science', 'took', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'computer']}, {'chunk_id': 7, 'text': 'graphics course introduced me to the fascinating world of graphics and visualization. I dived into\\n1 the theoretical aspects of computer science by studying courses on formal languages, automata\\ntheory, and the theory of computation. Additionally, I gained insights into the data aspects of\\ncomputer science through courses on database management systems and big data processing.\\nI also had a chance to collaborate with incredible professors, such as working with Professor', 'start_char': 3142, 'end_char': 3615, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['graphics', 'course', 'introduced', 'fascinating', 'world', 'graphics', 'visualization', 'dived', 'theoretical', 'aspects', 'computer', 'science', 'studying', 'courses', 'formal', 'languages', 'automata', 'theory', 'theory', 'computation', 'additionally', 'gained', 'insights', 'data', 'aspects', 'computer', 'science', 'courses', 'database', 'management', 'systems', 'big', 'data', 'processing', 'also', 'chance', 'collaborate', 'incredible', 'professors', 'working', 'professor']}, {'chunk_id': 8, 'text': 'K.S. Rao, concerning my Bachelor’s project for a semester. The project was titled ”Developing an\\nEnd-to-End System for Accent Classification from Emotional Speech,” with particular emphasis\\non raw sound waveforms. By directly working with the unprocessed waveforms, we could capture\\nsubtle nuances and characteristics often overlooked in traditional spectral-based techniques. We\\nexplored different feature extraction methods, conducted thorough testing, and identified the opti-', 'start_char': 3616, 'end_char': 4095, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['rao', 'concerning', 'bachelor', 'project', 'semester', 'project', 'titled', 'developing', 'system', 'accent', 'classification', 'emotional', 'speech', 'particular', 'emphasis', 'raw', 'sound', 'waveforms', 'directly', 'working', 'unprocessed', 'waveforms', 'could', 'capture', 'subtle', 'nuances', 'characteristics', 'often', 'overlooked', 'traditional', 'techniques', 'explored', 'different', 'feature', 'extraction', 'methods', 'conducted', 'thorough', 'testing', 'identified']}, {'chunk_id': 9, 'text': 'mal approach. Our goal was accurate accent classification and gaining insights from the underlying\\nmechanisms of raw waveforms. Working closely with Professor K.S. Rao throughout the project\\nwas a privilege. His expertise and guidance significantly enchanced my understanding of speech\\nanalysis and the broader field of computer science.\\nContinuing my journey in computer science, I was driven to deepen my understanding and', 'start_char': 4096, 'end_char': 4520, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['mal', 'approach', 'goal', 'accurate', 'accent', 'classification', 'gaining', 'insights', 'underlying', 'mechanisms', 'raw', 'waveforms', 'working', 'closely', 'professor', 'rao', 'throughout', 'project', 'privilege', 'expertise', 'guidance', 'significantly', 'enchanced', 'understanding', 'speech', 'analysis', 'broader', 'field', 'computer', 'science', 'continuing', 'journey', 'computer', 'science', 'driven', 'deepen', 'understanding']}, {'chunk_id': 10, 'text': 'bridge the gap between academia and industry, and I actively pursued opportunities to collaborate\\nwith professionals in the field. In pursuit of this goal, I applied for an internship at Nvidia, where I\\nhad the chance to explore the realm of system-level designing and embedded systems. During my\\ninternship, I thoroughly studied the register specifications manuals of the Nvidia Orin System on\\nChip (SOC) and devised a programming sequence based on the acquired knowledge. Subsequently,', 'start_char': 4521, 'end_char': 5008, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['bridge', 'gap', 'academia', 'industry', 'actively', 'pursued', 'opportunities', 'collaborate', 'professionals', 'field', 'pursuit', 'goal', 'applied', 'internship', 'nvidia', 'chance', 'explore', 'realm', 'designing', 'embedded', 'systems', 'internship', 'thoroughly', 'studied', 'register', 'specifications', 'manuals', 'nvidia', 'orin', 'system', 'chip', 'soc', 'devised', 'programming', 'sequence', 'based', 'acquired', 'knowledge', 'subsequently']}, {'chunk_id': 11, 'text': 'I used it to design and implement a QNX resource manager for the TKE Watchdog timer on Orin\\nSOC. I created a client daemon to validate the basic functionalities, such as start, stop and ping.\\nWe had to change the device tree and rebuild the entire database to test additional safety modes,\\nsuch as windowed and challenge-response modes. I have created parameters to access the device\\ntree to test it efficiently. Working with a team of eight highly qualified developers with extensive', 'start_char': 5009, 'end_char': 5493, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['used', 'design', 'implement', 'qnx', 'resource', 'manager', 'tke', 'watchdog', 'timer', 'orin', 'soc', 'created', 'client', 'daemon', 'validate', 'basic', 'functionalities', 'start', 'stop', 'ping', 'change', 'device', 'tree', 'rebuild', 'entire', 'database', 'test', 'additional', 'safety', 'modes', 'windowed', 'modes', 'created', 'parameters', 'access', 'device', 'tree', 'test', 'efficiently', 'working', 'team', 'eight', 'highly', 'qualified', 'developers', 'extensive']}, {'chunk_id': 12, 'text': 'collaboration helped me improve my communication and teamwork skills.\\nAfter delving into the vast and diverse fields of computer science and collaborating with highly\\nknowledgeable individuals, my passion for the subject has driven me to pursue further exploration\\nand mastery. While standing at a crossroads and deciding my future, I became aware of the ex-\\nceptional academic and research opportunities in computer science offered at [..XX..] University.', 'start_char': 5494, 'end_char': 5950, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['collaboration', 'helped', 'improve', 'communication', 'teamwork', 'skills', 'delving', 'vast', 'diverse', 'fields', 'computer', 'science', 'collaborating', 'highly', 'knowledgeable', 'individuals', 'passion', 'subject', 'driven', 'pursue', 'exploration', 'mastery', 'standing', 'crossroads', 'deciding', 'future', 'became', 'aware', 'ceptional', 'academic', 'research', 'opportunities', 'computer', 'science', 'offered', 'xx', 'university']}, {'chunk_id': 13, 'text': 'The prospect of learning from distinguished professors like [..XX..] and [..XX..] fills me with\\nanticipation and eagerness.\\nUpon completing my post-graduation, I envision myself thoroughly equipped with the neces-\\nsary skills and resources to drive innovation and make meaningful global contributions. I aim to\\nacquire a comprehensive skill set and knowledge base that will enable me to excel in various aspects', 'start_char': 5951, 'end_char': 6362, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['prospect', 'learning', 'distinguished', 'professors', 'like', 'xx', 'xx', 'fills', 'anticipation', 'eagerness', 'upon', 'completing', 'envision', 'thoroughly', 'equipped', 'sary', 'skills', 'resources', 'drive', 'innovation', 'make', 'meaningful', 'global', 'contributions', 'aim', 'acquire', 'comprehensive', 'skill', 'set', 'knowledge', 'base', 'enable', 'excel', 'various', 'aspects']}, {'chunk_id': 14, 'text': 'of the field, from software development and data analysis to artificial intelligence and emerging\\ntechnologies. I look forward to joining [..XX..] to achieve my dreams and do impactful work.\\n2', 'start_char': 6363, 'end_char': 6555, 'metadata': {}, 'context': None, 'embedding': None, 'tokenized_text': ['field', 'software', 'development', 'data', 'analysis', 'artificial', 'intelligence', 'emerging', 'technologies', 'look', 'forward', 'joining', 'xx', 'achieve', 'dreams', 'impactful', 'work']}], 'faiss_index': <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x1681731e0> >, 'bm25_index': <rank_bm25.BM25Okapi object at 0x165097f10>}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed documents: {processed_docs}\")\n",
    "processed_docs = processor.documents\n",
    "print(f\"Processed documents: {processed_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing retriever...\n",
      "INFO:__main__:\n",
      "Testing _initialize_indices...\n",
      "INFO:__main__:Index initialization successful\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initializing retriever...\")\n",
    "retriever = ChainOfThoughtRetriever(\n",
    "    documents=processed_docs,\n",
    "    embedding_model=processor.embedding_model,\n",
    "    anthropic_client=processor.client,\n",
    "    device='cpu'\n",
    ")\n",
    "# 1. Test _initialize_indices\n",
    "logger.info(\"\\nTesting _initialize_indices...\")\n",
    "# This was called during initialization, let's verify the structures\n",
    "assert len(retriever.all_chunks) > 0, \"Chunks were not initialized\"\n",
    "assert len(retriever.doc_indices) > 0, \"Document indices were not initialized\"\n",
    "assert len(retriever.bm25_indices) > 0, \"BM25 indices were not initialized\"\n",
    "logger.info(\"Index initialization successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Testing _get_dense_results...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\n",
      "INFO:__main__:Found 3 dense results\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\nTesting _get_dense_results...\")\n",
    "dense_results = await retriever._get_dense_results(\n",
    "    query=\"what are the use cases?\",\n",
    "    k=3\n",
    ")\n",
    "logger.info(f\"Found {len(dense_results)} dense results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result #22\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Data Requirements:\n",
      "•Access to a diverse set of loc...\n",
      "\n",
      "Search Result #20\n",
      "Score: 0.032\n",
      "Source: 1.pdf\n",
      "Text: Challenges and Risks\n",
      "•Computational Limitations: G...\n",
      "\n",
      "Search Result #12\n",
      "Score: -1.401\n",
      "Source: 1.pdf\n",
      "Text: operations.\n",
      "Architecture/Process:\n",
      "1. Data Ingestio...\n"
     ]
    }
   ],
   "source": [
    "for result in dense_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "\n",
    "def _extract_key_concepts( query: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Extract important concepts and terms from a query using NLP techniques.\n",
    "    \n",
    "    This function uses multiple approaches to identify key concepts:\n",
    "    1. Named entity recognition\n",
    "    2. Noun phrase extraction\n",
    "    3. Important keyword identification\n",
    "    \"\"\"\n",
    "    key_concepts = set()\n",
    "    \n",
    "    # Tokenize and tag parts of speech\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Extract noun phrases using chunking\n",
    "    grammar = \"\"\"\n",
    "        NP: {<DT>?<JJ>*<NN.*>+}     # Chunk determiners, adjectives, and nouns\n",
    "        CP: {<JJR|JJS><IN><NN.*>+}  # Comparative phrases\n",
    "    \"\"\"\n",
    "    chunk_parser = RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(pos_tags)\n",
    "    \n",
    "    # Extract concepts from noun phrases\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() in {'NP', 'CP'}):\n",
    "        concept = ' '.join(word for word, tag in subtree.leaves())\n",
    "        if len(concept.split()) > 1:  # Only keep multi-word concepts\n",
    "            key_concepts.add(concept)\n",
    "    \n",
    "    # Add single important terms (nouns, verbs, adjectives)\n",
    "    important_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'JJ'}\n",
    "    for word, tag in pos_tags:\n",
    "        if tag[:2] in important_tags and len(word) > 3:\n",
    "            key_concepts.add(word)\n",
    "    \n",
    "    return key_concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def _construct_enhanced_query(\n",
    "    original_query: str,\n",
    "    key_concepts: Set[str],\n",
    "    max_concepts: int = 3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Construct an enhanced search query that combines the original query with key concepts.\n",
    "    \n",
    "    The function creates a more comprehensive query by:\n",
    "    1. Keeping the original query intent\n",
    "    2. Adding the most relevant discovered concepts\n",
    "    3. Maintaining a natural language structure\n",
    "    \"\"\"\n",
    "    # Remove concepts that are already in the original query\n",
    "    original_lower = original_query.lower()\n",
    "    new_concepts = {\n",
    "        concept for concept in key_concepts\n",
    "        if concept.lower() not in original_lower\n",
    "    }\n",
    "    \n",
    "    # Ensure the embedding model is available\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    # Score concepts by relevance to original query\n",
    "    concept_scores = {}\n",
    "    for concept in new_concepts:\n",
    "        # Calculate semantic similarity using embeddings\n",
    "        concept_embedding = embedding_model.encode([concept])[0]\n",
    "        query_embedding = embedding_model.encode([original_query])[0]\n",
    "        similarity = cosine_similarity(\n",
    "            concept_embedding.reshape(1, -1),\n",
    "            query_embedding.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # Score also considers concept specificity\n",
    "        specificity = len(concept.split())  # Multi-word concepts are usually more specific\n",
    "        concept_scores[concept] = similarity * (1 + 0.1 * specificity)\n",
    "    \n",
    "    # Select top concepts\n",
    "    top_concepts = sorted(\n",
    "        concept_scores.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:max_concepts]\n",
    "    \n",
    "    # Construct enhanced query\n",
    "    if original_query.lower().startswith(('what', 'who', 'where', 'when', 'why', 'how')):\n",
    "        # For question queries, append concepts naturally\n",
    "        enhanced_query = f\"{original_query} considering {', '.join(c[0] for c in top_concepts)}\"\n",
    "    else:\n",
    "        # For keyword queries, combine with AND logic\n",
    "        enhanced_query = f\"{original_query} AND ({' OR '.join(c[0] for c in top_concepts)})\"\n",
    "    \n",
    "    return enhanced_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cases', 'the use cases'}\n",
      "what are the use cases? considering \n"
     ]
    }
   ],
   "source": [
    "concepts = _extract_key_concepts(\"what are the use cases?\")\n",
    "print(concepts)\n",
    "print(_construct_enhanced_query(\"what are the use cases?\", concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/battalavamshi/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "import spacy\n",
    "from dateparser import parse as date_parse\n",
    "\n",
    "class TemporalQueryParser:\n",
    "    \"\"\"\n",
    "    Parses natural language queries to extract temporal expressions and constraints.\n",
    "    \n",
    "    This class handles various ways users might express time in their queries:\n",
    "    - Relative times (\"last week\", \"past 3 days\")\n",
    "    - Specific dates (\"since January 1st\")\n",
    "    - Time periods (\"between March and April\")\n",
    "    - Informal expressions (\"recent\", \"latest\", \"new\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load spaCy model for natural language processing\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Define temporal pattern matching rules\n",
    "        self.relative_patterns = {\n",
    "            # Last/past period patterns\n",
    "            r'last\\s+(\\d+)\\s+(day|week|month|year)s?': self._handle_last_n_period,\n",
    "            r'past\\s+(\\d+)\\s+(day|week|month|year)s?': self._handle_last_n_period,\n",
    "            r'previous\\s+(\\d+)\\s+(day|week|month|year)s?': self._handle_last_n_period,\n",
    "            \n",
    "            # Single period patterns\n",
    "            r'last (day|week|month|year)': self._handle_last_period,\n",
    "            r'yesterday': self._handle_yesterday,\n",
    "            r'today': self._handle_today,\n",
    "            \n",
    "            # Since patterns\n",
    "            r'since\\s+(.+?)(?=\\s+and|\\s+or|$)': self._handle_since,\n",
    "            \n",
    "            # Between patterns\n",
    "            r'between\\s+(.+?)\\s+and\\s+(.+?)(?=\\s+and|\\s+or|$)': self._handle_between\n",
    "        }\n",
    "        \n",
    "        # Patterns for informal temporal expressions\n",
    "        self.informal_patterns = {\n",
    "            'recent': timedelta(days=7),    # Consider \"recent\" as last 7 days\n",
    "            'latest': timedelta(days=3),    # \"latest\" as last 3 days\n",
    "            'new': timedelta(days=1),       # \"new\" as last 24 hours\n",
    "            'current': timedelta(days=1)    # \"current\" as last 24 hours\n",
    "        }\n",
    "\n",
    "    def parse_temporal_query(self, query: str) -> Tuple[str, Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"\n",
    "        Extract temporal constraints from a natural language query and return the cleaned query\n",
    "        along with start and end dates.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query with potential temporal expressions\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (cleaned query, start_date, end_date)\n",
    "        \"\"\"\n",
    "        # Start with current time as reference\n",
    "        now = datetime.now()\n",
    "        start_date = None\n",
    "        end_date = now\n",
    "        \n",
    "        # Process query with spaCy for better linguistic understanding\n",
    "        doc = self.nlp(query)\n",
    "        \n",
    "        # First, check for explicit patterns\n",
    "        for pattern, handler in self.relative_patterns.items():\n",
    "            matches = re.finditer(pattern, query, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                # Extract temporal information using the appropriate handler\n",
    "                temp_start, temp_end = handler(match)\n",
    "                if temp_start:\n",
    "                    # If we find multiple temporal expressions, use the most restrictive\n",
    "                    start_date = temp_start if not start_date else max(start_date, temp_start)\n",
    "                    end_date = temp_end if temp_end else now\n",
    "                # Remove the temporal expression from query\n",
    "                query = query.replace(match.group(0), '').strip()\n",
    "\n",
    "        # Check for informal temporal expressions\n",
    "        for term, delta in self.informal_patterns.items():\n",
    "            if term in query.lower():\n",
    "                start_date = now - delta\n",
    "                query = re.sub(r'\\b' + term + r'\\b', '', query, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # Additional check for date entities using spaCy\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ('DATE', 'TIME'):\n",
    "                parsed_date = date_parse(ent.text)\n",
    "                if parsed_date:\n",
    "                    # Use as start date if it's a single date reference\n",
    "                    start_date = parsed_date\n",
    "                    query = query.replace(ent.text, '').strip()\n",
    "\n",
    "        return query.strip(), start_date, end_date\n",
    "\n",
    "    def _handle_last_n_period(self, match) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Handle patterns like 'last 3 days', 'past 2 weeks'\"\"\"\n",
    "        now = datetime.now()\n",
    "        number = int(match.group(1))\n",
    "        period = match.group(2).lower()\n",
    "        \n",
    "        if period == 'day':\n",
    "            delta = timedelta(days=number)\n",
    "        elif period == 'week':\n",
    "            delta = timedelta(weeks=number)\n",
    "        elif period == 'month':\n",
    "            delta = timedelta(days=number * 30)  # Approximate\n",
    "        else:  # year\n",
    "            delta = timedelta(days=number * 365)  # Approximate\n",
    "            \n",
    "        return now - delta, now\n",
    "\n",
    "    def _handle_last_period(self, match) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Handle patterns like 'last week', 'last month'\"\"\"\n",
    "        now = datetime.now()\n",
    "        period = match.group(1).lower()\n",
    "        \n",
    "        if period == 'day':\n",
    "            delta = timedelta(days=1)\n",
    "        elif period == 'week':\n",
    "            delta = timedelta(weeks=1)\n",
    "        elif period == 'month':\n",
    "            delta = timedelta(days=30)  # Approximate\n",
    "        else:  # year\n",
    "            delta = timedelta(days=365)  # Approximate\n",
    "            \n",
    "        return now - delta, now\n",
    "\n",
    "    def _handle_yesterday(self, match) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Handle 'yesterday' references\"\"\"\n",
    "        now = datetime.now()\n",
    "        start = now - timedelta(days=1)\n",
    "        start = start.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end = start + timedelta(days=1)\n",
    "        return start, end\n",
    "\n",
    "    def _handle_today(self, match) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Handle 'today' references\"\"\"\n",
    "        now = datetime.now()\n",
    "        start = now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        return start, now\n",
    "\n",
    "    def _handle_since(self, match) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Handle 'since' expressions\"\"\"\n",
    "        date_str = match.group(1)\n",
    "        parsed_date = date_parse(date_str)\n",
    "        return parsed_date, None\n",
    "\n",
    "    def _handle_between(self, match) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Handle 'between' expressions\"\"\"\n",
    "        start_str = match.group(1)\n",
    "        end_str = match.group(2)\n",
    "        start_date = date_parse(start_str)\n",
    "        end_date = date_parse(end_str)\n",
    "        return start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "from dateparser import parse as date_parse\n",
    "\n",
    "class TemporalQueryParser:\n",
    "    \"\"\"\n",
    "    Parses natural language queries to extract temporal expressions and constraints.\n",
    "    Uses regex patterns and dateparser for robust temporal understanding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define temporal pattern matching rules with clear, descriptive names\n",
    "        self.relative_patterns = {\n",
    "            # Patterns for specific time periods with numbers\n",
    "            'numbered_period': r'(?:last|past|previous)\\s+(\\d+)\\s+(day|week|month|year)s?',\n",
    "            \n",
    "            # Patterns for single time periods\n",
    "            'single_period': r'last (day|week|month|year)',\n",
    "            \n",
    "            # Patterns for specific time references\n",
    "            'specific_day': r'yesterday|today|tomorrow',\n",
    "            \n",
    "            # Patterns for time ranges\n",
    "            'since_pattern': r'since\\s+(.+?)(?=\\s+and|\\s+or|$)',\n",
    "            'between_pattern': r'between\\s+(.+?)\\s+and\\s+(.+?)(?=\\s+and|\\s+or|$)',\n",
    "            \n",
    "            # Patterns for relative time expressions\n",
    "            'relative_time': r'(\\d+)\\s+(day|week|month|year)s?\\s+ago'\n",
    "        }\n",
    "        \n",
    "        # Map informal temporal expressions to timedeltas\n",
    "        self.informal_patterns = {\n",
    "            'recent': timedelta(days=7),    # \"recent\" → last 7 days\n",
    "            'latest': timedelta(days=3),    # \"latest\" → last 3 days\n",
    "            'new': timedelta(days=1),       # \"new\" → last 24 hours\n",
    "            'current': timedelta(days=1)    # \"current\" → last 24 hours\n",
    "        }\n",
    "\n",
    "    def parse_temporal_query(self, query: str) -> Tuple[str, Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"\n",
    "        Extracts temporal constraints from a natural language query.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query that might contain temporal expressions\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - Cleaned query with temporal expressions removed\n",
    "            - Start date (None if not specified)\n",
    "            - End date (defaults to current time if not specified)\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        start_date = None\n",
    "        end_date = now\n",
    "        working_query = query.lower()  # Work with lowercase for easier matching\n",
    "\n",
    "        # Handle numbered periods (e.g., \"last 3 days\")\n",
    "        numbered_matches = re.search(self.relative_patterns['numbered_period'], working_query)\n",
    "        if numbered_matches:\n",
    "            number = int(numbered_matches.group(1))\n",
    "            period = numbered_matches.group(2)\n",
    "            start_date = self._calculate_period_start(number, period)\n",
    "            working_query = re.sub(self.relative_patterns['numbered_period'], '', working_query)\n",
    "\n",
    "        # Handle single periods (e.g., \"last week\")\n",
    "        single_match = re.search(self.relative_patterns['single_period'], working_query)\n",
    "        if single_match and not start_date:  # Only if no start date set\n",
    "            period = single_match.group(1)\n",
    "            start_date = self._calculate_period_start(1, period)\n",
    "            working_query = re.sub(self.relative_patterns['single_period'], '', working_query)\n",
    "\n",
    "        # Handle specific day references\n",
    "        day_match = re.search(self.relative_patterns['specific_day'], working_query)\n",
    "        if day_match and not start_date:\n",
    "            day_ref = day_match.group(0)\n",
    "            start_date, end_date = self._handle_specific_day(day_ref)\n",
    "            working_query = re.sub(day_ref, '', working_query)\n",
    "\n",
    "        # Handle informal temporal expressions\n",
    "        for term, delta in self.informal_patterns.items():\n",
    "            if term in working_query:\n",
    "                start_date = now - delta\n",
    "                working_query = re.sub(r'\\b' + term + r'\\b', '', working_query)\n",
    "\n",
    "        # Clean up the query\n",
    "        cleaned_query = ' '.join(working_query.split())\n",
    "        \n",
    "        return cleaned_query, start_date, end_date\n",
    "\n",
    "    def _calculate_period_start(self, number: int, period: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Calculates the start date based on a number and time period.\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        if period == 'day':\n",
    "            return now - timedelta(days=number)\n",
    "        elif period == 'week':\n",
    "            return now - timedelta(weeks=number)\n",
    "        elif period == 'month':\n",
    "            return now - timedelta(days=number * 30)  # Approximate\n",
    "        else:  # year\n",
    "            return now - timedelta(days=number * 365)  # Approximate\n",
    "\n",
    "    def _handle_specific_day(self, day_ref: str) -> Tuple[datetime, datetime]:\n",
    "        \"\"\"\n",
    "        Handles specific day references like 'yesterday', 'today'.\n",
    "        Returns start and end timestamps for the day.\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        if day_ref == 'yesterday':\n",
    "            start = now - timedelta(days=1)\n",
    "        elif day_ref == 'today':\n",
    "            start = now\n",
    "        else:  # tomorrow\n",
    "            start = now + timedelta(days=1)\n",
    "            \n",
    "        # Set to beginning of the day\n",
    "        start = start.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end = start + timedelta(days=1)\n",
    "        return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: show me documents from last week about project updates\n",
      "Cleaned: show me documents from about project updates\n",
      "Time range: 2024-11-23 17:58:20.725466 to 2024-11-30 17:58:20.724718\n",
      "\n",
      "Original: what changed in the system in the past 3 days\n",
      "Cleaned: what changed in the system in the\n",
      "Time range: 2024-11-27 17:58:20.725922 to 2024-11-30 17:58:20.725909\n",
      "\n",
      "Original: find meeting notes since March 1st\n",
      "Cleaned: find meeting notes since march 1st\n",
      "Time range: None to 2024-11-30 17:58:20.725955\n",
      "\n",
      "Original: get recent updates about the database\n",
      "Cleaned: get updates about the database\n",
      "Time range: 2024-11-23 17:58:20.726018 to 2024-11-30 17:58:20.726018\n",
      "\n",
      "Original: show changes between January and February\n",
      "Cleaned: show changes between january and february\n",
      "Time range: None to 2024-11-30 17:58:20.726112\n",
      "\n",
      "Original: find yesterday's deployment logs\n",
      "Cleaned: find 's deployment logs\n",
      "Time range: 2024-11-29 00:00:00 to 2024-11-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = TemporalQueryParser()\n",
    "queries = [\n",
    "    \"show me documents from last week about project updates\",\n",
    "    \"what changed in the system in the past 3 days\",\n",
    "    \"find meeting notes since March 1st\",\n",
    "    \"get recent updates about the database\",\n",
    "    \"show changes between January and February\",\n",
    "    \"find yesterday's deployment logs\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    cleaned_query, start_date, end_date = parser.parse_temporal_query(query)\n",
    "    print(f\"\\nOriginal: {query}\")\n",
    "    print(f\"Cleaned: {cleaned_query}\")\n",
    "    print(f\"Time range: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: QueryType.REASONING\n",
      "Confidence: 0.2551538050174713\n",
      "Weights: {'dense': 0.5510307610034942, 'sparse': 0.44896923899650576}\n",
      "QueryType.FACTUAL: 0.279\n",
      "QueryType.REASONING: 0.181\n",
      "QueryType.COMPARISON: 0.185\n",
      "QueryType.EXPLORATORY: 0.179\n",
      "QueryType.PROCEDURAL: 0.177\n"
     ]
    }
   ],
   "source": [
    "from temp import PretrainedQueryClassifier\n",
    "# Initialize the classifier with the pre-trained model\n",
    "classifier = PretrainedQueryClassifier()\n",
    "\n",
    "# Analyze a query\n",
    "result = classifier.analyze_query(\"how does machine learning work\")\n",
    "print(f\"Query Type: {result.query_type}\")\n",
    "print(f\"Confidence: {result.confidence}\")\n",
    "print(f\"Weights: {result.weights}\")\n",
    "\n",
    "# Get detailed similarity scores\n",
    "scores = classifier.get_similarity_scores(\"what is capital of France\")\n",
    "for query_type, score in scores.items():\n",
    "    print(f\"{query_type}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import spacy\n",
    "import logging\n",
    "from enum import Enum\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QueryType(Enum):\n",
    "    FACTUAL = \"factual\"         # Looking for specific facts or definitions\n",
    "    REASONING = \"reasoning\"      # Seeking explanations or understanding\n",
    "    COMPARISON = \"comparison\"    # Wanting to compare or contrast\n",
    "    EXPLORATORY = \"exploratory\" # Open-ended information seeking\n",
    "    PROCEDURAL = \"procedural\"   # How-to and step-by-step instructions\n",
    "\n",
    "@dataclass\n",
    "class QueryAnalysis:\n",
    "    \"\"\"Stores the analysis results for a query\"\"\"\n",
    "    query_type: QueryType\n",
    "    weights: Dict[str, float]\n",
    "    confidence: float = 1.0\n",
    "    features: Dict[str, float] = None\n",
    "\n",
    "class IntentBasedClassifier:\n",
    "    \"\"\"\n",
    "    Classifies queries based on linguistic patterns and semantic markers\n",
    "    that indicate search intent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load spaCy's English language model for linguistic analysis\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            # If model isn't installed, download it\n",
    "            import subprocess\n",
    "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Intent markers for different query types\n",
    "        self.intent_patterns = {\n",
    "            QueryType.FACTUAL: {\n",
    "                'question_words': ['what', 'when', 'where', 'who', 'which'],\n",
    "                'verbs': ['is', 'are', 'was', 'were', 'does'],\n",
    "                'patterns': ['define', 'meaning of', 'definition of']\n",
    "            },\n",
    "            QueryType.REASONING: {\n",
    "                'question_words': ['why', 'how'],\n",
    "                'verbs': ['explain', 'causes', 'affects', 'influences', 'works'],\n",
    "                'patterns': ['reason for', 'because', 'explain', 'understand']\n",
    "            },\n",
    "            QueryType.COMPARISON: {\n",
    "                'markers': ['compare', 'versus', 'vs', 'difference', 'better', 'worse'],\n",
    "                'patterns': ['compared to', 'differences between', 'pros and cons'],\n",
    "                'conjunctions': ['and', 'or', 'vs']\n",
    "            },\n",
    "            QueryType.EXPLORATORY: {\n",
    "                'verbs': ['tell', 'describe', 'elaborate', 'discuss'],\n",
    "                'patterns': ['tell me about', 'what are', 'information about', 'learn about'],\n",
    "                'markers': ['overview', 'introduction', 'basics']\n",
    "            },\n",
    "            QueryType.PROCEDURAL: {\n",
    "                'markers': ['how to', 'steps', 'guide', 'tutorial', 'instructions'],\n",
    "                'verbs': ['make', 'create', 'build', 'implement', 'setup', 'configure'],\n",
    "                'patterns': ['way to', 'process of', 'method of']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Weights for different retrieval methods based on query type\n",
    "        self.retrieval_weights = {\n",
    "            QueryType.FACTUAL: {\n",
    "                'dense': 0.35,   # Favor keyword matching for facts\n",
    "                'sparse': 0.65\n",
    "            },\n",
    "            QueryType.REASONING: {\n",
    "                'dense': 0.75,   # Heavily favor semantic for explanations\n",
    "                'sparse': 0.25\n",
    "            },\n",
    "            QueryType.COMPARISON: {\n",
    "                'dense': 0.60,   # Balance for comparing entities\n",
    "                'sparse': 0.40\n",
    "            },\n",
    "            QueryType.EXPLORATORY: {\n",
    "                'dense': 0.80,   # Strong semantic preference for exploration\n",
    "                'sparse': 0.20\n",
    "            },\n",
    "            QueryType.PROCEDURAL: {\n",
    "                'dense': 0.55,   # Slight semantic preference for steps\n",
    "                'sparse': 0.45\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _extract_linguistic_features(self, doc) -> Dict[str, bool]:\n",
    "        \"\"\"Extract linguistic features from the spaCy document.\"\"\"\n",
    "        return {\n",
    "            'has_wh_question': any(token.tag_ == 'WDT' or token.tag_ == 'WP' or token.tag_ == 'WRB' for token in doc),\n",
    "            'has_comparison': any(token.dep_ == 'amod' for token in doc),\n",
    "            'has_action_verb': any(token.pos_ == 'VERB' and token.dep_ != 'aux' for token in doc),\n",
    "            'has_conjunction': any(token.dep_ == 'cc' for token in doc),\n",
    "            'is_command': doc[0].pos_ == 'VERB'\n",
    "        }\n",
    "\n",
    "    def _calculate_type_scores(\n",
    "        self,\n",
    "        query: str,\n",
    "        doc,\n",
    "        features: Dict[str, bool]\n",
    "    ) -> Dict[QueryType, float]:\n",
    "        \"\"\"\n",
    "        Calculate scores for each query type based on linguistic features\n",
    "        and intent patterns.\n",
    "        \"\"\"\n",
    "        scores = {qt: 0.0 for qt in QueryType}\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check each query type's patterns\n",
    "        for query_type, patterns in self.intent_patterns.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check question words and verbs\n",
    "            for key in ['question_words', 'verbs', 'markers']:\n",
    "                if key in patterns:\n",
    "                    score += sum(word in query_lower.split() \n",
    "                               for word in patterns[key]) * 0.3\n",
    "            \n",
    "            # Check multi-word patterns\n",
    "            if 'patterns' in patterns:\n",
    "                score += sum(pattern in query_lower \n",
    "                           for pattern in patterns['patterns']) * 0.5\n",
    "            \n",
    "            # Add linguistic feature scores\n",
    "            if query_type == QueryType.FACTUAL and features['has_wh_question']:\n",
    "                score += 0.4\n",
    "            elif query_type == QueryType.REASONING and 'why' in query_lower:\n",
    "                score += 0.6\n",
    "            elif query_type == QueryType.COMPARISON and features['has_comparison']:\n",
    "                score += 0.4\n",
    "            elif query_type == QueryType.PROCEDURAL and features['is_command']:\n",
    "                score += 0.4\n",
    "            \n",
    "            scores[query_type] = min(score, 1.0)  # Normalize to 0-1\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def analyze_query(self, query: str) -> QueryAnalysis:\n",
    "        \"\"\"\n",
    "        Analyze a query to determine its type and appropriate retrieval weights.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query to analyze\n",
    "            \n",
    "        Returns:\n",
    "            QueryAnalysis object containing type, weights, and confidence\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Process query with spaCy\n",
    "            doc = self.nlp(query)\n",
    "            \n",
    "            # Extract linguistic features\n",
    "            features = self._extract_linguistic_features(doc)\n",
    "            \n",
    "            # Calculate scores for each query type\n",
    "            type_scores = self._calculate_type_scores(query, doc, features)\n",
    "            \n",
    "            # Get the highest scoring type\n",
    "            predicted_type = max(type_scores.items(), key=lambda x: x[1])\n",
    "            query_type = predicted_type[0]\n",
    "            confidence = predicted_type[1]\n",
    "            \n",
    "            # Get base weights for this type\n",
    "            weights = self.retrieval_weights[query_type].copy()\n",
    "            \n",
    "            # Adjust weights if confidence is low\n",
    "            if confidence < 0.5:\n",
    "                # Move weights closer to balanced (0.5/0.5)\n",
    "                for key in weights:\n",
    "                    weights[key] = 0.5 + (weights[key] - 0.5) * confidence\n",
    "            \n",
    "            return QueryAnalysis(\n",
    "                query_type=query_type,\n",
    "                weights=weights,\n",
    "                confidence=confidence,\n",
    "                features=type_scores\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing query: {str(e)}\")\n",
    "            # Return safe defaults\n",
    "            return QueryAnalysis(\n",
    "                query_type=QueryType.EXPLORATORY,\n",
    "                weights={'dense': 0.5, 'sparse': 0.5},\n",
    "                confidence=0.0\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Classification Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Query: what is machine learning\n",
      "Type: factual\n",
      "Confidence: 1.000\n",
      "Weights: {'dense': 0.35, 'sparse': 0.65}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 1.000\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.000\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.000\n",
      "\n",
      "Query: what's the capital of france?\n",
      "Type: factual\n",
      "Confidence: 0.400\n",
      "Weights: {'dense': 0.44, 'sparse': 0.56}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.400\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.000\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.000\n",
      "\n",
      "Query: why do neural networks work well for image recognition\n",
      "Type: reasoning\n",
      "Confidence: 0.900\n",
      "Weights: {'dense': 0.75, 'sparse': 0.25}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.400\n",
      "  reasoning: 0.900\n",
      "  comparison: 0.400\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.000\n",
      "\n",
      "Query: compare supervised and unsupervised learning\n",
      "Type: comparison\n",
      "Confidence: 0.700\n",
      "Weights: {'dense': 0.6, 'sparse': 0.4}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.000\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.700\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.400\n",
      "\n",
      "Query: tell me about deep learning\n",
      "Type: exploratory\n",
      "Confidence: 0.800\n",
      "Weights: {'dense': 0.8, 'sparse': 0.2}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.000\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.400\n",
      "  exploratory: 0.800\n",
      "  procedural: 0.400\n",
      "\n",
      "Query: how to implement a neural network in Python\n",
      "Type: factual\n",
      "Confidence: 0.400\n",
      "Weights: {'dense': 0.44, 'sparse': 0.56}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.400\n",
      "  reasoning: 0.300\n",
      "  comparison: 0.400\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.300\n",
      "\n",
      "Query: what causes overfitting in machine learning models\n",
      "Type: factual\n",
      "Confidence: 0.700\n",
      "Weights: {'dense': 0.35, 'sparse': 0.65}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.700\n",
      "  reasoning: 0.300\n",
      "  comparison: 0.000\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.000\n",
      "\n",
      "Query: steps to preprocess data for machine learning\n",
      "Type: procedural\n",
      "Confidence: 0.300\n",
      "Weights: {'dense': 0.515, 'sparse': 0.485}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.000\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.000\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.300\n",
      "\n",
      "Query: difference between CNN and RNN\n",
      "Type: comparison\n",
      "Confidence: 0.300\n",
      "Weights: {'dense': 0.53, 'sparse': 0.47000000000000003}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.000\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.300\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.000\n",
      "\n",
      "Query: explain backpropagation algorithm\n",
      "Type: reasoning\n",
      "Confidence: 0.800\n",
      "Weights: {'dense': 0.75, 'sparse': 0.25}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.000\n",
      "  reasoning: 0.800\n",
      "  comparison: 0.000\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.400\n",
      "\n",
      "Query: guide to training deep learning models\n",
      "Type: procedural\n",
      "Confidence: 0.700\n",
      "Weights: {'dense': 0.55, 'sparse': 0.45}\n",
      "\n",
      "Scores for each type:\n",
      "  factual: 0.000\n",
      "  reasoning: 0.000\n",
      "  comparison: 0.400\n",
      "  exploratory: 0.000\n",
      "  procedural: 0.700\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifier\n",
    "classifier = IntentBasedClassifier()\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"what is machine learning\",\n",
    "    \"what's the capital of france?\",\n",
    "    \"why do neural networks work well for image recognition\",\n",
    "    \"compare supervised and unsupervised learning\",\n",
    "    \"tell me about deep learning\",\n",
    "    \"how to implement a neural network in Python\",\n",
    "    \"what causes overfitting in machine learning models\",\n",
    "    \"steps to preprocess data for machine learning\",\n",
    "    \"difference between CNN and RNN\",\n",
    "    \"explain backpropagation algorithm\",\n",
    "    \"guide to training deep learning models\"\n",
    "]\n",
    "\n",
    "print(\"\\nQuery Classification Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    analysis = classifier.analyze_query(query)\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Type: {analysis.query_type.value}\")\n",
    "    print(f\"Confidence: {analysis.confidence:.3f}\")\n",
    "    print(f\"Weights: {analysis.weights}\")\n",
    "    \n",
    "    if analysis.features:\n",
    "        print(\"\\nScores for each type:\")\n",
    "        for qtype, score in analysis.features.items():\n",
    "            print(f\"  {qtype.value}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EfficientTokenizer()\n",
    "\n",
    "def preprocess_text_worker(text: str) -> List[str]:\n",
    "    \"\"\"Worker function for text preprocessing\"\"\"\n",
    "    try:\n",
    "        return tokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in preprocessing worker: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class CustomBM25:\n",
    "    \"\"\"\n",
    "    Enhanced BM25 implementation with improved technical term handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.tokenized_corpus = [preprocess_text_worker(doc) for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "        # Build vocabulary with frequency information\n",
    "        self.vocabulary = {}\n",
    "        for doc in self.tokenized_corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary[token] = self.vocabulary.get(token, 0) + 1\n",
    "    \n",
    "    def get_scores(self, query: str) -> np.ndarray:\n",
    "        query_tokens = preprocess_text_worker(query)\n",
    "        return self.bm25.get_scores(query_tokens)\n",
    "    \n",
    "    def get_matching_terms(self, query: str) -> Dict[str, int]:\n",
    "        \"\"\"Get matching terms and their corpus frequencies for debugging\"\"\"\n",
    "        query_tokens = preprocess_text_worker(query)\n",
    "        return {\n",
    "            token: self.vocabulary.get(token, 0)\n",
    "            for token in query_tokens\n",
    "            if token in self.vocabulary\n",
    "        }\n",
    "        \n",
    "    def get_document_terms(self, doc_idx: int) -> List[str]:\n",
    "        \"\"\"Get the tokenized terms for a specific document\"\"\"\n",
    "        return self.tokenized_corpus[doc_idx] if 0 <= doc_idx < len(self.tokenized_corpus) else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'nv', 'embed', 'v1.2.3', 'with', 'pytorch']\n",
      "['use', 'word2vec', 'and', 'bert', 'for', 'nlp']\n",
      "['requirement', 'python>=3.6', 'tensorflow<=2.0']\n",
      "['contact', 'support@example.com', 'or', 'visit', 'https://example.com']\n",
      "['use', 'c++', 'and', 'for', 'ml', 'ai']\n",
      "['pre-trained.model', 'and', 'word_embedding', 'work', 'well']\n",
      "['testing', 'camelcase', 'and', 'pascalcase']\n",
      "['u.s.a.', 'ph.d.', 'research', 'on', 'gpt-3.5']\n"
     ]
    }
   ],
   "source": [
    "# from preprocessing import CustomBM25, preprocess_text_worker\n",
    "import re\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "def preprocess_batch_worker(texts: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Worker function for batch text preprocessing\"\"\"\n",
    "    try:\n",
    "        return tokenizer.batch_tokenize(texts)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in batch preprocessing worker: {str(e)}\")\n",
    "        return [[] for _ in texts]\n",
    "\n",
    "def test_preprocessing():\n",
    "    test_cases = [\n",
    "        \"Testing NV-embed v1.2.3 with PyTorch\",\n",
    "        \"Using word2vec and BERT for NLP\",\n",
    "        \"Requirements: Python>=3.6, TensorFlow<=2.0\",\n",
    "        \"Contact support@example.com or visit https://example.com\",\n",
    "        \"Using C++ and C# for ML/AI\",\n",
    "        \"pre-trained.models and word_embeddings work well\",\n",
    "        \"Testing camelCase and PascalCase\",\n",
    "        \"U.S.A. Ph.D. research on GPT-3.5\",\n",
    "    ]\n",
    "    \n",
    "    results = preprocess_batch_worker(test_cases)\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        \n",
    "\n",
    "test_preprocessing()\n",
    "\n",
    "# test_bm25()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
